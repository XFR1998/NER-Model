{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# bi-lstm+crf，使用pytorch-crf库实现crf，可cuda加速。\n",
    "\n",
    "数据集说明：\n",
    "\n",
    "1: B-BANK 代表银行实体的开始\n",
    "\n",
    "2: I-BANK 代表银行实体的内部\n",
    "\n",
    "3: B-PRODUCT 代表产品实体的开始\n",
    "\n",
    "4: I-PRODUCT 代表产品实体的内部\n",
    "\n",
    "5: O 代表不属于标注的范围\n",
    "\n",
    "6: B-COMMENTS_N 代表用户评论（名词）\n",
    "\n",
    "7: I-COMMENTS_N 代表用户评论（名词）实体的内部\n",
    "\n",
    "8: B-COMMENTS_ADJ 代表用户评论（形容词）\n",
    "\n",
    "9: I-COMMENTS_ADJ 代表用户评论（形容词）实体的内部"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda能用\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchcrf import CRF\n",
    "torch.manual_seed(1)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('{}能用'.format(device))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "data = pd.read_csv('./train_data_public.csv')\n",
    "data.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "train_data, valid_data = train_test_split(data, test_size = 0.2, random_state=42)\n",
    "train_data.index = list(range(len(train_data)))\n",
    "valid_data.index = list(range(len(valid_data)))\n",
    "test_data = pd.read_csv('./test_public.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 8000 entries, 0 to 7999\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   text        8000 non-null   object\n",
      " 1   BIO_anno    8000 non-null   object\n",
      " 2   class       8000 non-null   int64 \n",
      " 3   bank_topic  6124 non-null   object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 312.5+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2000 entries, 0 to 1999\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   text        2000 non-null   object\n",
      " 1   BIO_anno    2000 non-null   object\n",
      " 2   class       2000 non-null   int64 \n",
      " 3   bank_topic  1512 non-null   object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 78.1+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5093 entries, 0 to 5092\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      5093 non-null   int64 \n",
      " 1   text    5093 non-null   object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 79.7+ KB\n"
     ]
    }
   ],
   "source": [
    "train_data.info()\n",
    "valid_data.info()\n",
    "test_data.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "              text                                           BIO_anno  class  \\\n0       封卡只是鸡的原因吗？          B-COMMENTS_N I-COMMENTS_N O O O O O O O O      2   \n1  销卡后45天后等征信更新再申请  B-COMMENTS_N I-COMMENTS_N O O O O O O B-PRODUC...      2   \n2        有8w多的，招行的                        O O O O O O B-BANK I-BANK O      2   \n\n  bank_topic  \n0       交通银行  \n1       建设银行  \n2        NaN  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>BIO_anno</th>\n      <th>class</th>\n      <th>bank_topic</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>封卡只是鸡的原因吗？</td>\n      <td>B-COMMENTS_N I-COMMENTS_N O O O O O O O O</td>\n      <td>2</td>\n      <td>交通银行</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>销卡后45天后等征信更新再申请</td>\n      <td>B-COMMENTS_N I-COMMENTS_N O O O O O O B-PRODUC...</td>\n      <td>2</td>\n      <td>建设银行</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>有8w多的，招行的</td>\n      <td>O O O O O O B-BANK I-BANK O</td>\n      <td>2</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\ML_ENVS\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "F:\\ML_ENVS\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "F:\\ML_ENVS\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "F:\\ML_ENVS\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# 把text和标注按单个字分隔开，放进列表\n",
    "train_data['BIO_anno'] = train_data['BIO_anno'].apply(lambda x:x.split(' '))\n",
    "valid_data['BIO_anno'] = valid_data['BIO_anno'].apply(lambda x:x.split(' '))\n",
    "# 将text和标注组合存进元组\n",
    "train_data['training_data'] = train_data.apply(lambda row: [list(row['text']), row['BIO_anno']], axis=1)\n",
    "valid_data['validating_data'] = valid_data.apply(lambda row: [list(row['text']), row['BIO_anno']], axis=1)\n",
    "test_data['testing_data'] = test_data.apply(lambda row: list(row['text']), axis=1)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "num = train_data['training_data'].apply(lambda x:type(x[0])!=type([]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num.sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集大小： 8000\n",
      "验证集大小： 2000\n",
      "测试集大小： 5093\n"
     ]
    }
   ],
   "source": [
    "training_data_txt = train_data['training_data'].to_list()\n",
    "validating_data_txt = valid_data['validating_data'].to_list()\n",
    "testing_data_txt = test_data['testing_data'].to_list()\n",
    "print('训练集大小：',len(training_data_txt))\n",
    "print('验证集大小：',len(validating_data_txt))\n",
    "print('测试集大小：',len(testing_data_txt))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "# 定义一些工具函数\n",
    "\n",
    "# 句子转idx\n",
    "def prepare_sequence(seq, word2idx):\n",
    "    idxs = [word2idx[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "def argmax(vec):\n",
    "    # return the argmax as a python int\n",
    "    # 返回vec的dim为1维度上的最大值索引\n",
    "    _, idx = torch.max(vec,axis=1)\n",
    "    return idx.item()\n",
    "\n",
    "# Compute log sum exp in a numerically stable way for the forward algorithm\n",
    "# 前向算法是不断累积之前的结果，这样就会有个缺点\n",
    "# 指数和累积到一定程度后，会超过计算机浮点值的最大值，变成inf，这样取log后也是inf\n",
    "# 为了避免这种情况，用一个合适的值clip去提指数和的公因子，这样就不会使某项变得过大而无法计算\n",
    "# SUM = log(exp(s1)+exp(s2)+...+exp(s100))\n",
    "#     = log{exp(clip)*[exp(s1-clip)+exp(s2-clip)+...+exp(s100-clip)]}\n",
    "#     = clip + log[exp(s1-clip)+exp(s2-clip)+...+exp(s100-clip)]\n",
    "# where clip=max\n",
    "def log_sum_exp(vec):\n",
    "    max_score = vec[0, argmax(vec)]\n",
    "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
    "    return max_score + torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# 定义网络结构：bi-lstm + crf\n",
    "class BiLSTM_CRF(nn.Module):\n",
    "    def __init__(self, vocab_size, tag2idx, embedding_dim, hidden_dim):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tag2idx = tag2idx\n",
    "        self.tagset_size = len(tag2idx)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeds = nn.Embedding(vocab_size, self.embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim,\n",
    "                            hidden_size=self.hidden_dim//2, # 双向lstm，最后拼接后就是hidden_dim了。\n",
    "                            num_layers=1,\n",
    "                            bidirectional=True)\n",
    "        # 将BiLSTM提取的特征向量映射到特征空间，即经过全连接得到发射分数\n",
    "        self.hidden2tag = nn.Linear(self.hidden_dim, self.tagset_size)\n",
    "\n",
    "        # 转移矩阵的参数初始化，transitions[i,j]代表的是从第j个tag转移到第i个tag的转移分数\n",
    "        self.transitions = nn.Parameter(torch.randn(self.tagset_size, self.tagset_size))\n",
    "\n",
    "        # 初始化所有其他tag转移到START_TAG的分数非常小，即不可能由其他tag转移到START_TAG\n",
    "        # 初始化STOP_TAG转移到所有其他tag的分数非常小，即不可能由STOP_TAG转移到其他tag\n",
    "        self.transitions.data[tag2idx[START_TAG], :] = -10000\n",
    "        self.transitions.data[:, tag2idx[STOP_TAG]] = -10000\n",
    "\n",
    "        self.hidden = self.init_hidden()\n",
    "    def init_hidden(self):\n",
    "        # 初始化lstm参数\n",
    "        # h0, c0的shape: (num_layers*2, bs, hidden_size), 双向就乘2\n",
    "        h0 = torch.randn(2, 1, self.hidden_dim//2)\n",
    "        c0 = torch.randn(2, 1, self.hidden_dim//2)\n",
    "        return (h0, c0)\n",
    "\n",
    "    def _get_lstm_features(self, sentence):\n",
    "        # 通过bi-lstm提取特征\n",
    "        self.hidden = self.init_hidden()\n",
    "        embeds = self.word_embeds(sentence).view(len(sentence),1,-1)\n",
    "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
    "        print(lstm_out.shape)\n",
    "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
    "        print(lstm_out.shape)\n",
    "        lstm_feats = self.hidden2tag(lstm_out)\n",
    "        print(lstm_feats.shape)\n",
    "        return lstm_feats\n",
    "\n",
    "    def _score_sentence(self, feats, tags):\n",
    "        # 计算给定tag序列的分数，即一条路径的分数\n",
    "        score = torch.zeros(1)\n",
    "        tags = torch.cat([torch.tensor([self.tag2idx[START_TAG]], dtype=torch.long), tags])\n",
    "        for i, feat in enumerate(feats):\n",
    "            # 递推计算路径分数：转移分数 + 发射分数\n",
    "            score = score + self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
    "        score = score + self.transitions[self.tag2idx[STOP_TAG], tags[-1]]\n",
    "        return score\n",
    "\n",
    "    def _forward_alg(self, feats):\n",
    "        # 通过前向算法递推计算\n",
    "        init_alphas = torch.full((1, self.tagset_size), -10000.)\n",
    "        # 初始化step 0即START位置的发射分数，START_TAG取0其他位置取-10000\n",
    "        init_alphas[0][self.tag2idx[START_TAG]] = 0.\n",
    "\n",
    "        # 将初始化START位置为0的发射分数赋值给previous\n",
    "        previous = init_alphas\n",
    "\n",
    "        # 迭代整个句子\n",
    "        for obs in feats:\n",
    "            # 当前时间步的前向tensor\n",
    "            alphas_t = []\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # 取出当前tag的发射分数，与之前时间步的tag无关\n",
    "                emit_score = obs[next_tag].view(1, -1).expand(1, self.tagset_size)\n",
    "                # 取出当前tag由之前tag转移过来的转移分数\n",
    "                trans_score = self.transitions[next_tag].view(1, -1)\n",
    "                # 当前路径的分数：之前时间步分数 + 转移分数 + 发射分数\n",
    "                next_tag_var = previous + trans_score + emit_score\n",
    "                # 对当前分数取log-sum-exp\n",
    "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
    "            # 更新previous 递推计算下一个时间步\n",
    "            previous = torch.cat(alphas_t).view(1, -1)\n",
    "        # 考虑最终转移到STOP_TAG\n",
    "        terminal_var = previous + self.transitions[self.tag2idx[STOP_TAG]]\n",
    "        # 计算最终的分数\n",
    "        scores = log_sum_exp(terminal_var)\n",
    "        return scores\n",
    "\n",
    "\n",
    "    def _viterbi_decode(self, feats):\n",
    "        backpointers = []\n",
    "\n",
    "        # 初始化viterbi的previous变量\n",
    "        init_vvars = torch.full((1, self.tagset_size), -10000.)\n",
    "        init_vvars[0][self.tag2idx[START_TAG]] = 0\n",
    "\n",
    "        previous = init_vvars\n",
    "        for obs in feats:\n",
    "            # 保存当前时间步的回溯指针\n",
    "            bptrs_t = []\n",
    "            # 保存当前时间步的viterbi变量\n",
    "            viterbivars_t = []\n",
    "\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # 维特比算法记录最优路径时只考虑上一步的分数以及上一步tag转移到当前tag的转移分数\n",
    "                # 并不取决与当前tag的发射分数\n",
    "                next_tag_var = previous + self.transitions[next_tag]\n",
    "                best_tag_id = argmax(next_tag_var)\n",
    "                bptrs_t.append(best_tag_id)\n",
    "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
    "            # 更新previous，加上当前tag的发射分数obs\n",
    "            previous = (torch.cat(viterbivars_t) + obs).view(1, -1)\n",
    "            # 回溯指针记录当前时间步各个tag来源前一步的tag\n",
    "            backpointers.append(bptrs_t)\n",
    "\n",
    "        # Transition to STOP_TAG\n",
    "        # 考虑转移到STOP_TAG的转移分数\n",
    "        terminal_var = previous + self.transitions[self.tag2idx[STOP_TAG]]\n",
    "        best_tag_id = argmax(terminal_var)\n",
    "        path_score = terminal_var[0][best_tag_id]\n",
    "\n",
    "        # 通过回溯指针解码出最优路径\n",
    "        best_path = [best_tag_id]\n",
    "        # best_tag_id作为线头，反向遍历backpointers找到最优路径\n",
    "        for bptrs_t in reversed(backpointers):\n",
    "            best_tag_id = bptrs_t[best_tag_id]\n",
    "            best_path.append(best_tag_id)\n",
    "        # 去除START_TAG\n",
    "        start = best_path.pop()\n",
    "        assert start == self.tag2idx[START_TAG]  # Sanity check\n",
    "        best_path.reverse()\n",
    "        return path_score, best_path\n",
    "\n",
    "    def neg_log_likelihood(self, sentence, tags):\n",
    "        # CRF损失函数由两部分组成，真实路径的分数和所有路径的总分数。\n",
    "        # 真实路径的分数应该是所有路径中分数最高的。\n",
    "        # log真实路径的分数/log所有可能路径的分数，越大越好，构造crf loss函数取反，loss越小越好\n",
    "        feats = self._get_lstm_features(sentence)\n",
    "        forward_score = self._forward_alg(feats)\n",
    "        gold_score = self._score_sentence(feats, tags)\n",
    "        return forward_score - gold_score\n",
    "\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        # 通过BiLSTM提取发射分数\n",
    "        lstm_feats = self._get_lstm_features(sentence)\n",
    "        # 根据发射分数以及转移分数，通过viterbi解码找到一条最优路径\n",
    "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
    "        return score, tag_seq"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Model, self).__init__()\n",
    "        self.config = config\n",
    "        # if config.embedding_pretrained is not None:\n",
    "        #     self.embedding = nn.Embedding.from_pretrained(config.embedding_pretrained,\n",
    "        #                                                   freeze=False)  # 表示训练过程词嵌入向量会更新\n",
    "        # else:\n",
    "        self.embedding = nn.Embedding(self.config.vocab_len, self.config.embedding_dim,\n",
    "                                      padding_idx=self.configs.word2idx['<PAD>'])  # PAD索引填充\n",
    "\n",
    "        if self.config.bidirectional:\n",
    "            self.num_directions = 2\n",
    "        else:\n",
    "            self.num_directions = 1\n",
    "\n",
    "\n",
    "        self.rnn = nn.LSTM(input_size=self.config.embedding_dim,\n",
    "                           hidden_size=self.config.hidden_size,\n",
    "                           num_layers=self.config.num_layers,\n",
    "                           batch_first=True,\n",
    "                           bidirectional=self.config.bidirectional)\n",
    "\n",
    "        self.tag2idx = configs.tag2idx\n",
    "\n",
    "        # 转换参数矩阵 输入i,j是得分从j转换到i\n",
    "        self.tagset_size = len(self.tag2idx)\n",
    "        # 将lstm的输出映射到标记空间\n",
    "        self.hidden2tag = nn.Linear(self.config.hidden_size*self.num_directions, self.tagset_size)  # -> (B, num_class+2)  加上了START END\n",
    "        self.crf = CRF(num_tags=self.tagset_size,batch_first=True)\n",
    "\n",
    "    def _forward_alg(self, feats):\n",
    "        # 使用前向算法计算分区函数\n",
    "        init_alphas = self._make_tensor(torch.full((1, self.tagset_size), -10000.))\n",
    "        # START_TAG 包含所有得分\n",
    "        init_alphas[0][self.tag2idx[START_TAG]] = 0.\n",
    "\n",
    "        # 包装一个变量 以便获得自动反向提升\n",
    "        forward_var = init_alphas\n",
    "\n",
    "        # 通过句子迭代\n",
    "        for feat in feats:\n",
    "            alphas_t = []  # the forward tensor at this timestep\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # 广播发射得分：无论之前的标记是怎样的都是相同的\n",
    "                emit_score = feat[next_tag].view(1, -1).expand(1, self.tagset_size)\n",
    "                # trans_score 的第i个条目是从i转移到next_tag的分数\n",
    "                trans_score = self.transitions[next_tag].view(1, -1)\n",
    "                # next_tag_var 的第i个条目是执行log-sum-exp之前的变（i -> next_tag）的值\n",
    "                next_tag_var = forward_var + trans_score + emit_score\n",
    "                # 此标记的转发变量是所有分数的log-sum-exp\n",
    "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
    "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
    "        terminal_var = forward_var + self.transitions[self.tag2idx[STOP_TAG]]\n",
    "        alpha = log_sum_exp(terminal_var)\n",
    "        return alpha\n",
    "\n",
    "    def _score_sentence(self, feats, tags):\n",
    "        # Give the score of a provided tag sequence\n",
    "        score = self._make_tensor(torch.zeros(1))\n",
    "        tags = self._make_tensor(torch.cat([self._make_tensor(torch.tensor([self.tag2idx[START_TAG]], dtype=torch.long)),tags]))\n",
    "        for i, feat in enumerate(feats):\n",
    "            score = score + self.transitions[tags[i+1], tags[i]]+feat[tags[i+1]]\n",
    "        score = score + self.transitions[self.tag2idx[STOP_TAG], tags[-1]]\n",
    "        return score\n",
    "\n",
    "    def _viterbi_decode(self, feats):\n",
    "        backpointers = []\n",
    "\n",
    "        # Initialize the viterbi variables in log space\n",
    "        init_vvars = self._make_tensor(torch.full((1, self.tagset_size), -10000.))\n",
    "        init_vvars[0][self.tag2idx[START_TAG]] = 0\n",
    "\n",
    "        # forward_var at step o holds the viterbi variables for step i-1\n",
    "        forward_var = init_vvars\n",
    "        for feat in feats:\n",
    "            bptrs_t = []  # hold the backpointers for this step\n",
    "            viterbivars_t = []  # holds the viterbi variables for this step\n",
    "\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # next_tag_var[i]保存上一步的标签i的viterbi变量\n",
    "                # 加上标签i转换到next_tag的分数 我们这里不包括emission分数 因为最大值不依赖于它们（在下面添加它们）\n",
    "                next_tag_var = forward_var + self.transitions[next_tag]\n",
    "                best_tag_id = argmax(next_tag_var)\n",
    "                bptrs_t.append(best_tag_id)\n",
    "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
    "            # 现在添加emission分数 并将forward_var分配给刚计算的viterbi变量集\n",
    "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
    "            backpointers.append(bptrs_t)\n",
    "\n",
    "        # 过渡到STOP_TAG\n",
    "        terminal_var = forward_var + self.transitions[self.tag2idx[STOP_TAG]]\n",
    "        best_tag_id = argmax(terminal_var)\n",
    "        path_score = terminal_var[0][best_tag_id]\n",
    "\n",
    "        # 按照后退指针解码最佳路径\n",
    "        best_path = [best_tag_id]\n",
    "        for bptrs_t in reversed(backpointers):\n",
    "            best_tag_id = bptrs_t[best_tag_id]\n",
    "            best_path.append(best_tag_id)\n",
    "        # 弹出开始标记（我们不想将器返回给调用者）\n",
    "        start = best_path.pop()\n",
    "        assert start == self.tag2idx[START_TAG]  # Sanity check\n",
    "        best_path.reverse()\n",
    "        return path_score, best_path\n",
    "\n",
    "    def _get_lstm_features(self, x):\n",
    "        # 数据预处理时，x被处理成是一个tuple,其内容是: (word, label).\n",
    "        # x:b_size\n",
    "        x = self.embedding(x)  # B -> (B, e_d)\n",
    "        x = x.unsqueeze(1)  # (B, e_b) -> (B, 1, e_b)\n",
    "        h_0, c_0 = self._init_hidden(batchs=x.size(0))\n",
    "        out, (hidden, c) = self.rnn(x,(h_0, c_0))  # out:(B, 1, num_directions*hidden_size) hidden:(num_layer*nun_directions, B,  hidden_size)\n",
    "        # out = out.squeeze(1)\n",
    "        # output is batch_first but hidden not\n",
    "        out = self.hidden2tag(out)  # (B,num_directions*hidden_size) -> (B, num_class)\n",
    "        out = out.transpose(0, 1)\n",
    "        return out\n",
    "\n",
    "    def neg_log_likelihood(self, x, tags):  # 损失函数\n",
    "        tags = tags.unsqueeze(0)\n",
    "        feats = self._get_lstm_features(x)\n",
    "        return -self.crf(feats, tags)\n",
    "\n",
    "\n",
    "\n",
    "    def _init_hidden(self, batchs):  # 初始化h_0和c_0 与GRU不同的是多了c_0（细胞状态）\n",
    "        h_0 = torch.zeros(self.config.num_layers*self.num_directions, batchs,  self.config.hidden_size)\n",
    "        c_0 = torch.zeros(self.config.num_layers*self.num_directions, batchs, self.config.hidden_size)\n",
    "        return self._make_tensor(h_0), self._make_tensor(c_0)\n",
    "\n",
    "    def _make_tensor(self, tensor):\n",
    "        # 函数说明： 将传入的tensor转移到cpu或gpu内\n",
    "\n",
    "        tensor_ret = tensor.to(self.config.device)\n",
    "        return tensor_ret\n",
    "\n",
    "    # def getTagLs(self, config):\n",
    "    #     tag_ls = config.class_ls\n",
    "    #     tag_ls.append(\"<START>\")\n",
    "    #     tag_ls.append(\"<STOP>\")\n",
    "    #     return tag_ls\n",
    "    #\n",
    "    # def getTagDic(self):\n",
    "    #     tag_dic = {}\n",
    "    #     for idx, label in enumerate(self.tag_ls):\n",
    "    #         tag_dic[label] = idx\n",
    "    #     return tag_dic\n",
    "    #\n",
    "    # def idx2Tag(self, idx):\n",
    "    #     return self.tag_ls[idx]\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 数据预处理时，x被处理成是一个tuple,其内容是: (word, label).\n",
    "        # x:b_size\n",
    "        lstm_feats = self._get_lstm_features(x)  # 获取BiLSTM的emission分数\n",
    "\n",
    "        out = self.crf.decode(lstm_feats)\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "from utils.param_configs import Configs\n",
    "configs = Configs()\n",
    "\n",
    "# 将训练集汉字使用数字表示\n",
    "# 为了方便调试，先用100条数据进行训练，调试好后可用全量数据进行训练\n",
    "# training_data_txt = training_data_txt[:]\n",
    "# --------------------------建立字典，字: idx-------------------------------------\n",
    "word2idx = {}\n",
    "# 训练集的\n",
    "for sentence, tags in training_data_txt:\n",
    "    for word in sentence:\n",
    "        if word not in word2idx:\n",
    "            word2idx[word] = len(word2idx)\n",
    "\n",
    "# 验证集的\n",
    "for sentence, tags in validating_data_txt:\n",
    "    for word in sentence:\n",
    "        if word not in word2idx:\n",
    "            word2idx[word] = len(word2idx)\n",
    "\n",
    "# 测试集的\n",
    "testing_data = testing_data_txt\n",
    "for sentence in testing_data:\n",
    "    for word in sentence:\n",
    "        if word not in word2idx:\n",
    "            word2idx[word] = len(word2idx)\n",
    "\n",
    "# 加2个特殊字符\n",
    "word2idx['<UNK>'] = len(word2idx)\n",
    "word2idx['<PAD>'] = len(word2idx)\n",
    "\n",
    "configs.word2idx = word2idx\n",
    "# ------------------------------------------------------------------------------"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "8000"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data_txt[0][0]),len(training_data_txt[0][1])\n",
    "len(training_data_txt)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "from utils.data_process import create_data_loader\n",
    "train_data_loader = create_data_loader(training_data_txt, configs)\n",
    "valid_data_loader = create_data_loader(validating_data_txt, configs)\n",
    "# test_data_loader = create_data_loader(testing_data_txt, configs) # 没有标签的测试集就不这样构建，因为没有label"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "(500, 125)"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data_loader),len(valid_data_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence': ['封 卡 只 是 鸡 的 原 因 吗 ？ <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>', '销 卡 后 4 5 天 后 等 征 信 更 新 再 申 请 <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>', '有 8 w 多 的 ， 招 行 的 <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>', '判 决 金 额 8 . 2 ， 表 示 你 需 要 还 款 8 . 2 ， 至 于 你 是 一 次 性 还 款 ， 还 是 分 期 还 款 ， 如 果 判 决 书 里 没 有 具 体 龟 腚 ， 那 你 可 以 跟 银 行 协 商 分 期 还 款 。 . . . <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>', '我 的 - 个 人 资 料 - 联 系 地 址 ? ? ， 刚 瞅 了 眼 我 自 己 的 信 息 ， 好 像 也 乱 了 ， 我 都 怀 疑 前 几 天 e 分 期 一 直 秒 拒 就 是 因 为 办 储 蓄 卡 的 时 候 地 址 又 给 我 搞 乱 了 。 . . . <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>', '这 短 信 只 是 系 统 统 一 默 认 的 ， 短 信 提 示 制 卡 中 其 实 也 就 是 等 于 数 字 卡 正 在 生 成 中 ， 制 卡 完 成 也 就 是 生 成 完 成 这 意 思 ， 最 后 一 步 就 差 激 活 了 ！ . . . <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>', '我 固 定 额 度 3 0 0 0 0 , 现 金 分 期 1 2 0 0 0 0 ！ <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>', '求 哥 哥 帮 忙 。 到 了 这 个 界 面 还 会 被 拒 绝 吗 ？ 刷 脸 要 等 2 4 小 时 。 <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>', '最 近 我 刷 了 好 几 次 都 没 有 ， 机 器 换 了 几 台 也 没 有 封 卡 啊 <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>', '以 前 经 常 空 卡 ， 今 年 开 始 正 常 用 卡 ， 真 实 加 套 现 ， 留 2 0 % 以 上 额 度 ， 四 个 月 了 ， 还 没 动 静 <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>', '招 商 哪 个 大 功 能 需 要 盾 <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>', '应 该 可 以 吧 ， 我 的 资 质 一 般 ， 刚 毕 业 2 年 ， 招 行 卡 2 万 额 度 <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>', '现 在 毕 业 不 止 6 万 了 吗 ？ 我 自 然 升 到 6 万 两 三 年 了 也 没 动 静 <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>', '缓 冲 期 什 么 意 思 啊 ， 而 且 我 工 行 信 用 卡 一 个 月 都 花 不 了 3 0 0 块 钱 怎 么 还 是 5 星 级 <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>', '初 审 黑 炭 都 能 过 就 是 审 核 没 过 <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>', '正 常 用 卡 一 张 就 够 ? ? 留 两 张 额 度 高 的 以 防 日 后 有 资 金 需 求 卡 少 方 便 养 卡 <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>'], 'label': ['B-COMMENTS_N I-COMMENTS_N O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O', 'B-COMMENTS_N I-COMMENTS_N O O O O O O B-PRODUCT I-PRODUCT O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O', 'O O O O O O B-BANK I-BANK O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O', 'O O B-COMMENTS_N I-COMMENTS_N O O O O O O O O O B-COMMENTS_N I-COMMENTS_N O O O O O O O O O O O B-COMMENTS_N I-COMMENTS_N O O O B-PRODUCT I-PRODUCT B-COMMENTS_N I-COMMENTS_N O O O O O O O O O O O O O O O O O O O O O O O B-PRODUCT I-PRODUCT B-COMMENTS_N I-COMMENTS_N O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O', 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-COMMENTS_ADJ O O O O O O O O O O B-PRODUCT I-PRODUCT O O O B-COMMENTS_ADJ O O O O O B-PRODUCT I-PRODUCT I-PRODUCT O O O O O O O O O B-COMMENTS_ADJ O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O', 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-COMMENTS_N I-COMMENTS_N O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O', 'O O O B-COMMENTS_N I-COMMENTS_N O O O O O O O O B-PRODUCT I-PRODUCT O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O', 'O O O O O O O O O O O O O O O B-COMMENTS_ADJ I-COMMENTS_ADJ O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O', 'O O O O O O O O O O O O O O O O O O O O O B-COMMENTS_N I-COMMENTS_N O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O', 'O O O O O O O O O O O B-COMMENTS_ADJ I-COMMENTS_ADJ O O O O O O O O O O O O O O O B-COMMENTS_N I-COMMENTS_N O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O', 'B-BANK I-BANK O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O', 'O O O O O O O O O O O O O O O O O O O B-BANK I-BANK O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O', 'O O O O O O O O O O O O O O B-COMMENTS_ADJ O O O O O O O O B-COMMENTS_ADJ I-COMMENTS_ADJ I-COMMENTS_ADJ O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O', 'O O O O O O O O O O O O B-BANK I-BANK B-PRODUCT I-PRODUCT I-PRODUCT O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O', 'O O O O O O O O O B-COMMENTS_N I-COMMENTS_N O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O', 'B-COMMENTS_ADJ I-COMMENTS_ADJ O O O O O O O O O O O B-COMMENTS_N I-COMMENTS_N B-COMMENTS_ADJ O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O'], 'sentence_tensor': tensor([[   0,    1,    2,  ..., 2621, 2621, 2621],\n",
      "        [  10,    1,   11,  ..., 2621, 2621, 2621],\n",
      "        [  23,   24,   25,  ..., 2621, 2621, 2621],\n",
      "        ...,\n",
      "        [ 198,  199,   49,  ..., 2621, 2621, 2621],\n",
      "        [ 211,  212,  213,  ..., 2621, 2621, 2621],\n",
      "        [ 120,  161,  167,  ..., 2621, 2621, 2621]]), 'label_tensor': tensor([[5, 6, 0,  ..., 0, 0, 0],\n",
      "        [5, 6, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [7, 8, 0,  ..., 0, 0, 0]]), 'mask_tensor': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "for sample in train_data_loader:\n",
    "    print(sample)\n",
    "    break\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "625"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "configs.word2idx['封']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "['封 卡 只 是 鸡 的 原 因 吗 ？ <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>',\n '销 卡 后 4 5 天 后 等 征 信 更 新 再 申 请 <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>',\n '有 8 w 多 的 ， 招 行 的 <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>',\n '判 决 金 额 8 . 2 ， 表 示 你 需 要 还 款 8 . 2 ， 至 于 你 是 一 次 性 还 款 ， 还 是 分 期 还 款 ， 如 果 判 决 书 里 没 有 具 体 龟 腚 ， 那 你 可 以 跟 银 行 协 商 分 期 还 款 。 . . . <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>',\n '我 的 - 个 人 资 料 - 联 系 地 址 ? ? ， 刚 瞅 了 眼 我 自 己 的 信 息 ， 好 像 也 乱 了 ， 我 都 怀 疑 前 几 天 e 分 期 一 直 秒 拒 就 是 因 为 办 储 蓄 卡 的 时 候 地 址 又 给 我 搞 乱 了 。 . . . <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>',\n '这 短 信 只 是 系 统 统 一 默 认 的 ， 短 信 提 示 制 卡 中 其 实 也 就 是 等 于 数 字 卡 正 在 生 成 中 ， 制 卡 完 成 也 就 是 生 成 完 成 这 意 思 ， 最 后 一 步 就 差 激 活 了 ！ . . . <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>',\n '我 固 定 额 度 3 0 0 0 0 , 现 金 分 期 1 2 0 0 0 0 ！ <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>',\n '求 哥 哥 帮 忙 。 到 了 这 个 界 面 还 会 被 拒 绝 吗 ？ 刷 脸 要 等 2 4 小 时 。 <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>',\n '最 近 我 刷 了 好 几 次 都 没 有 ， 机 器 换 了 几 台 也 没 有 封 卡 啊 <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>',\n '以 前 经 常 空 卡 ， 今 年 开 始 正 常 用 卡 ， 真 实 加 套 现 ， 留 2 0 % 以 上 额 度 ， 四 个 月 了 ， 还 没 动 静 <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>',\n '招 商 哪 个 大 功 能 需 要 盾 <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>',\n '应 该 可 以 吧 ， 我 的 资 质 一 般 ， 刚 毕 业 2 年 ， 招 行 卡 2 万 额 度 <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>',\n '现 在 毕 业 不 止 6 万 了 吗 ？ 我 自 然 升 到 6 万 两 三 年 了 也 没 动 静 <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>',\n '缓 冲 期 什 么 意 思 啊 ， 而 且 我 工 行 信 用 卡 一 个 月 都 花 不 了 3 0 0 块 钱 怎 么 还 是 5 星 级 <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>',\n '初 审 黑 炭 都 能 过 就 是 审 核 没 过 <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>',\n '正 常 用 卡 一 张 就 够 ? ? 留 两 张 额 度 高 的 以 防 日 后 有 资 金 需 求 卡 少 方 便 养 卡 <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>']"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['sentence']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "['B-COMMENTS_N I-COMMENTS_N O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O',\n 'B-COMMENTS_N I-COMMENTS_N O O O O O O B-PRODUCT I-PRODUCT O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O',\n 'O O O O O O B-BANK I-BANK O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O',\n 'O O B-COMMENTS_N I-COMMENTS_N O O O O O O O O O B-COMMENTS_N I-COMMENTS_N O O O O O O O O O O O B-COMMENTS_N I-COMMENTS_N O O O B-PRODUCT I-PRODUCT B-COMMENTS_N I-COMMENTS_N O O O O O O O O O O O O O O O O O O O O O O O B-PRODUCT I-PRODUCT B-COMMENTS_N I-COMMENTS_N O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O',\n 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-COMMENTS_ADJ O O O O O O O O O O B-PRODUCT I-PRODUCT O O O B-COMMENTS_ADJ O O O O O B-PRODUCT I-PRODUCT I-PRODUCT O O O O O O O O O B-COMMENTS_ADJ O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O',\n 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-COMMENTS_N I-COMMENTS_N O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O',\n 'O O O B-COMMENTS_N I-COMMENTS_N O O O O O O O O B-PRODUCT I-PRODUCT O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O',\n 'O O O O O O O O O O O O O O O B-COMMENTS_ADJ I-COMMENTS_ADJ O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O',\n 'O O O O O O O O O O O O O O O O O O O O O B-COMMENTS_N I-COMMENTS_N O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O',\n 'O O O O O O O O O O O B-COMMENTS_ADJ I-COMMENTS_ADJ O O O O O O O O O O O O O O O B-COMMENTS_N I-COMMENTS_N O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O',\n 'B-BANK I-BANK O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O',\n 'O O O O O O O O O O O O O O O O O O O B-BANK I-BANK O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O',\n 'O O O O O O O O O O O O O O B-COMMENTS_ADJ O O O O O O O O B-COMMENTS_ADJ I-COMMENTS_ADJ I-COMMENTS_ADJ O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O',\n 'O O O O O O O O O O O O B-BANK I-BANK B-PRODUCT I-PRODUCT I-PRODUCT O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O',\n 'O O O O O O O O O B-COMMENTS_N I-COMMENTS_N O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O',\n 'B-COMMENTS_ADJ I-COMMENTS_ADJ O O O O O O O O O O O B-COMMENTS_N I-COMMENTS_N B-COMMENTS_ADJ O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O']"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['label']\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[   0,    1,    2,  ..., 2621, 2621, 2621],\n        [  10,    1,   11,  ..., 2621, 2621, 2621],\n        [  23,   24,   25,  ..., 2621, 2621, 2621],\n        ...,\n        [ 198,  199,   49,  ..., 2621, 2621, 2621],\n        [ 211,  212,  213,  ..., 2621, 2621, 2621],\n        [ 120,  161,  167,  ..., 2621, 2621, 2621]])"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['sentence_tensor']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0])"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['mask_tensor'][0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0])"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['label_tensor'][-6]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "START_TAG = \"<START>\"\n",
    "STOP_TAG = \"<STOP>\"\n",
    "EMBEDDING_DIM = 11\n",
    "HIDDEN_DIM = 6\n",
    "\n",
    "\n",
    "# 将训练集汉字使用数字表示\n",
    "# 为了方便调试，先用100条数据进行训练，调试好后可用全量数据进行训练\n",
    "training_data = training_data_txt[:]\n",
    "\n",
    "# --------------------------建立字典，字: idx-------------------------------------\n",
    "word2idx = {}\n",
    "# 训练集的\n",
    "for sentence, tags in training_data:\n",
    "    for word in sentence:\n",
    "        if word not in word2idx:\n",
    "            word2idx[word] = len(word2idx)\n",
    "# 测试集的\n",
    "testing_data = testing_data_txt\n",
    "for sentence in testing_data:\n",
    "    for word in sentence:\n",
    "        if word not in word2idx:\n",
    "            word2idx[word] = len(word2idx)\n",
    "# ------------------------------------------------------------------------------\n",
    "START_TAG = \"<START>\"\n",
    "STOP_TAG = \"<STOP>\"\n",
    "# 标签：idx\n",
    "tag2idx = { \"O\": 0, \"B-BANK\": 1, \"I-BANK\": 2, \"B-PRODUCT\":3,'I-PRODUCT':4,\n",
    "             'B-COMMENTS_N':5, 'I-COMMENTS_N':6, 'B-COMMENTS_ADJ':7,\n",
    "             'I-COMMENTS_ADJ':8, START_TAG: 9, STOP_TAG: 10}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = BiLSTM_CRF(len(word2idx), tag2idx, EMBEDDING_DIM, HIDDEN_DIM)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 检查下模型输入输出\n",
    "precheck_sent = prepare_sequence(training_data[0][0], word2idx)\n",
    "precheck_tags = torch.tensor([tag2idx[t] for t in training_data[0][1]], dtype=torch.long)\n",
    "print(model(precheck_sent))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import time\n",
    "t = time.time()\n",
    "# Make sure prepare_sequence from earlier in the LSTM section is loaded\n",
    "for epoch in range(100):\n",
    "    print('the',epoch,' epoch')\n",
    "    print(f'Time Taken: {round(time.time()-t)} seconds')\n",
    "    for sentence, tags in training_data:\n",
    "        # 第一步，pytorch梯度累积，需要清零梯度\n",
    "        model.zero_grad()\n",
    "\n",
    "        # 第二步，将输入转化为tensors\n",
    "        sentence_in = prepare_sequence(sentence, word2idx)\n",
    "        targets = torch.tensor([tag2idx[t] for t in tags], dtype=torch.long)\n",
    "\n",
    "        # 进行前向计算，取出crf loss\n",
    "        loss = model.neg_log_likelihood(sentence_in, targets)\n",
    "\n",
    "        # 第四步，计算loss，梯度，通过optimier更新参数\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 训练结束查看模型预测结果，对比观察模型是否学到\n",
    "# 标签：idx\n",
    "idx2tag = { 0:\"O\", 1:\"B-BANK\", 2:\"I-BANK\", 3:\"B-PRODUCT\",4:'I-PRODUCT',\n",
    "             5:'B-COMMENTS_N', 6:'I-COMMENTS_N', 7:'B-COMMENTS_ADJ',\n",
    "             8:'I-COMMENTS_ADJ', 9:START_TAG, 10:STOP_TAG}\n",
    "with torch.no_grad():\n",
    "    precheck_sent = prepare_sequence(training_data[4][0], word2idx)\n",
    "    print(model(precheck_sent))\n",
    "    a = model(precheck_sent) # model return score, tag_seq\n",
    "    # a = pd.Series(a)\n",
    "    print('句子为：', ''.join(training_data[4][0]))\n",
    "    print('实体标注结果为：', ' '.join([idx2tag[i] for i in a[1]]))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}