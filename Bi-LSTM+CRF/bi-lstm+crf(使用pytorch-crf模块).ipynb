{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# bi-lstm+crf，使用pytorch-crf库实现crf，可cuda加速。\n",
    "\n",
    "数据集说明：\n",
    "\n",
    "1: B-BANK 代表银行实体的开始\n",
    "\n",
    "2: I-BANK 代表银行实体的内部\n",
    "\n",
    "3: B-PRODUCT 代表产品实体的开始\n",
    "\n",
    "4: I-PRODUCT 代表产品实体的内部\n",
    "\n",
    "5: O 代表不属于标注的范围\n",
    "\n",
    "6: B-COMMENTS_N 代表用户评论（名词）\n",
    "\n",
    "7: I-COMMENTS_N 代表用户评论（名词）实体的内部\n",
    "\n",
    "8: B-COMMENTS_ADJ 代表用户评论（形容词）\n",
    "\n",
    "9: I-COMMENTS_ADJ 代表用户评论（形容词）实体的内部"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda能用\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchcrf import CRF\n",
    "torch.manual_seed(1)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('{}能用'.format(device))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./train_data_public.csv')\n",
    "train_data.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "test_data = pd.read_csv('./test_public.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   text        10000 non-null  object\n",
      " 1   BIO_anno    10000 non-null  object\n",
      " 2   class       10000 non-null  int64 \n",
      " 3   bank_topic  7636 non-null   object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 312.6+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5093 entries, 0 to 5092\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      5093 non-null   int64 \n",
      " 1   text    5093 non-null   object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 79.7+ KB\n"
     ]
    }
   ],
   "source": [
    "train_data.info()\n",
    "test_data.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                text  \\\n0  交行14年用过，半年准备提额，却直接被降到1Ｋ，半年期间只T过一次三千，其它全部真实消费，第...   \n1                                单标我有了，最近visa双标返现活动好   \n2                                        建设银行提额很慢的……   \n\n                                            BIO_anno  class bank_topic  \n0  B-BANK I-BANK O O O O O O O O O O B-COMMENTS_N...      0       建设银行  \n1  B-PRODUCT I-PRODUCT O O O O O O B-PRODUCT I-PR...      1       建设银行  \n2  B-BANK I-BANK I-BANK I-BANK B-COMMENTS_N I-COM...      0       建设银行  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>BIO_anno</th>\n      <th>class</th>\n      <th>bank_topic</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>交行14年用过，半年准备提额，却直接被降到1Ｋ，半年期间只T过一次三千，其它全部真实消费，第...</td>\n      <td>B-BANK I-BANK O O O O O O O O O O B-COMMENTS_N...</td>\n      <td>0</td>\n      <td>建设银行</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>单标我有了，最近visa双标返现活动好</td>\n      <td>B-PRODUCT I-PRODUCT O O O O O O B-PRODUCT I-PR...</td>\n      <td>1</td>\n      <td>建设银行</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>建设银行提额很慢的……</td>\n      <td>B-BANK I-BANK I-BANK I-BANK B-COMMENTS_N I-COM...</td>\n      <td>0</td>\n      <td>建设银行</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# 把text和标注按单个字分隔开，放进列表\n",
    "train_data['BIO_anno'] = train_data['BIO_anno'].apply(lambda x:x.split(' '))\n",
    "# 将text和标注组合存进元组\n",
    "train_data['training_data'] = train_data.apply(lambda row: [list(row['text']),row['BIO_anno']], axis=1)\n",
    "test_data['testing_data'] = test_data.apply(lambda row: list(row['text']), axis=1)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "num = train_data['training_data'].apply(lambda x:type(x[0])!=type([]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num.sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集大小： 10000 测试集大小： 5093\n"
     ]
    }
   ],
   "source": [
    "training_data_txt = train_data['training_data'].to_list()\n",
    "testing_data_txt = test_data['testing_data'].to_list()\n",
    "print('训练集大小：',len(training_data_txt), '测试集大小：',len(testing_data_txt))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# 定义一些工具函数\n",
    "\n",
    "# 句子转idx\n",
    "def prepare_sequence(seq, word2idx):\n",
    "    idxs = [word2idx[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "def argmax(vec):\n",
    "    # return the argmax as a python int\n",
    "    # 返回vec的dim为1维度上的最大值索引\n",
    "    _, idx = torch.max(vec,axis=1)\n",
    "    return idx.item()\n",
    "\n",
    "# Compute log sum exp in a numerically stable way for the forward algorithm\n",
    "# 前向算法是不断累积之前的结果，这样就会有个缺点\n",
    "# 指数和累积到一定程度后，会超过计算机浮点值的最大值，变成inf，这样取log后也是inf\n",
    "# 为了避免这种情况，用一个合适的值clip去提指数和的公因子，这样就不会使某项变得过大而无法计算\n",
    "# SUM = log(exp(s1)+exp(s2)+...+exp(s100))\n",
    "#     = log{exp(clip)*[exp(s1-clip)+exp(s2-clip)+...+exp(s100-clip)]}\n",
    "#     = clip + log[exp(s1-clip)+exp(s2-clip)+...+exp(s100-clip)]\n",
    "# where clip=max\n",
    "def log_sum_exp(vec):\n",
    "    max_score = vec[0, argmax(vec)]\n",
    "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
    "    return max_score + torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# 定义网络结构：bi-lstm + crf\n",
    "class BiLSTM_CRF(nn.Module):\n",
    "    def __init__(self, vocab_size, tag2idx, embedding_dim, hidden_dim):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tag2idx = tag2idx\n",
    "        self.tagset_size = len(tag2idx)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeds = nn.Embedding(vocab_size, self.embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim,\n",
    "                            hidden_size=self.hidden_dim//2, # 双向lstm，最后拼接后就是hidden_dim了。\n",
    "                            num_layers=1,\n",
    "                            bidirectional=True)\n",
    "        # 将BiLSTM提取的特征向量映射到特征空间，即经过全连接得到发射分数\n",
    "        self.hidden2tag = nn.Linear(self.hidden_dim, self.tagset_size)\n",
    "\n",
    "        # 转移矩阵的参数初始化，transitions[i,j]代表的是从第j个tag转移到第i个tag的转移分数\n",
    "        self.transitions = nn.Parameter(torch.randn(self.tagset_size, self.tagset_size))\n",
    "\n",
    "        # 初始化所有其他tag转移到START_TAG的分数非常小，即不可能由其他tag转移到START_TAG\n",
    "        # 初始化STOP_TAG转移到所有其他tag的分数非常小，即不可能由STOP_TAG转移到其他tag\n",
    "        self.transitions.data[tag2idx[START_TAG], :] = -10000\n",
    "        self.transitions.data[:, tag2idx[STOP_TAG]] = -10000\n",
    "\n",
    "        self.hidden = self.init_hidden()\n",
    "    def init_hidden(self):\n",
    "        # 初始化lstm参数\n",
    "        # h0, c0的shape: (num_layers*2, bs, hidden_size), 双向就乘2\n",
    "        h0 = torch.randn(2, 1, self.hidden_dim//2)\n",
    "        c0 = torch.randn(2, 1, self.hidden_dim//2)\n",
    "        return (h0, c0)\n",
    "\n",
    "    def _get_lstm_features(self, sentence):\n",
    "        # 通过bi-lstm提取特征\n",
    "        self.hidden = self.init_hidden()\n",
    "        embeds = self.word_embeds(sentence).view(len(sentence),1,-1)\n",
    "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
    "        print(lstm_out.shape)\n",
    "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
    "        print(lstm_out.shape)\n",
    "        lstm_feats = self.hidden2tag(lstm_out)\n",
    "        print(lstm_feats.shape)\n",
    "        return lstm_feats\n",
    "\n",
    "    def _score_sentence(self, feats, tags):\n",
    "        # 计算给定tag序列的分数，即一条路径的分数\n",
    "        score = torch.zeros(1)\n",
    "        tags = torch.cat([torch.tensor([self.tag2idx[START_TAG]], dtype=torch.long), tags])\n",
    "        for i, feat in enumerate(feats):\n",
    "            # 递推计算路径分数：转移分数 + 发射分数\n",
    "            score = score + self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
    "        score = score + self.transitions[self.tag2idx[STOP_TAG], tags[-1]]\n",
    "        return score\n",
    "\n",
    "    def _forward_alg(self, feats):\n",
    "        # 通过前向算法递推计算\n",
    "        init_alphas = torch.full((1, self.tagset_size), -10000.)\n",
    "        # 初始化step 0即START位置的发射分数，START_TAG取0其他位置取-10000\n",
    "        init_alphas[0][self.tag2idx[START_TAG]] = 0.\n",
    "\n",
    "        # 将初始化START位置为0的发射分数赋值给previous\n",
    "        previous = init_alphas\n",
    "\n",
    "        # 迭代整个句子\n",
    "        for obs in feats:\n",
    "            # 当前时间步的前向tensor\n",
    "            alphas_t = []\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # 取出当前tag的发射分数，与之前时间步的tag无关\n",
    "                emit_score = obs[next_tag].view(1, -1).expand(1, self.tagset_size)\n",
    "                # 取出当前tag由之前tag转移过来的转移分数\n",
    "                trans_score = self.transitions[next_tag].view(1, -1)\n",
    "                # 当前路径的分数：之前时间步分数 + 转移分数 + 发射分数\n",
    "                next_tag_var = previous + trans_score + emit_score\n",
    "                # 对当前分数取log-sum-exp\n",
    "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
    "            # 更新previous 递推计算下一个时间步\n",
    "            previous = torch.cat(alphas_t).view(1, -1)\n",
    "        # 考虑最终转移到STOP_TAG\n",
    "        terminal_var = previous + self.transitions[self.tag2idx[STOP_TAG]]\n",
    "        # 计算最终的分数\n",
    "        scores = log_sum_exp(terminal_var)\n",
    "        return scores\n",
    "\n",
    "\n",
    "    def _viterbi_decode(self, feats):\n",
    "        backpointers = []\n",
    "\n",
    "        # 初始化viterbi的previous变量\n",
    "        init_vvars = torch.full((1, self.tagset_size), -10000.)\n",
    "        init_vvars[0][self.tag2idx[START_TAG]] = 0\n",
    "\n",
    "        previous = init_vvars\n",
    "        for obs in feats:\n",
    "            # 保存当前时间步的回溯指针\n",
    "            bptrs_t = []\n",
    "            # 保存当前时间步的viterbi变量\n",
    "            viterbivars_t = []\n",
    "\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # 维特比算法记录最优路径时只考虑上一步的分数以及上一步tag转移到当前tag的转移分数\n",
    "                # 并不取决与当前tag的发射分数\n",
    "                next_tag_var = previous + self.transitions[next_tag]\n",
    "                best_tag_id = argmax(next_tag_var)\n",
    "                bptrs_t.append(best_tag_id)\n",
    "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
    "            # 更新previous，加上当前tag的发射分数obs\n",
    "            previous = (torch.cat(viterbivars_t) + obs).view(1, -1)\n",
    "            # 回溯指针记录当前时间步各个tag来源前一步的tag\n",
    "            backpointers.append(bptrs_t)\n",
    "\n",
    "        # Transition to STOP_TAG\n",
    "        # 考虑转移到STOP_TAG的转移分数\n",
    "        terminal_var = previous + self.transitions[self.tag2idx[STOP_TAG]]\n",
    "        best_tag_id = argmax(terminal_var)\n",
    "        path_score = terminal_var[0][best_tag_id]\n",
    "\n",
    "        # 通过回溯指针解码出最优路径\n",
    "        best_path = [best_tag_id]\n",
    "        # best_tag_id作为线头，反向遍历backpointers找到最优路径\n",
    "        for bptrs_t in reversed(backpointers):\n",
    "            best_tag_id = bptrs_t[best_tag_id]\n",
    "            best_path.append(best_tag_id)\n",
    "        # 去除START_TAG\n",
    "        start = best_path.pop()\n",
    "        assert start == self.tag2idx[START_TAG]  # Sanity check\n",
    "        best_path.reverse()\n",
    "        return path_score, best_path\n",
    "\n",
    "    def neg_log_likelihood(self, sentence, tags):\n",
    "        # CRF损失函数由两部分组成，真实路径的分数和所有路径的总分数。\n",
    "        # 真实路径的分数应该是所有路径中分数最高的。\n",
    "        # log真实路径的分数/log所有可能路径的分数，越大越好，构造crf loss函数取反，loss越小越好\n",
    "        feats = self._get_lstm_features(sentence)\n",
    "        forward_score = self._forward_alg(feats)\n",
    "        gold_score = self._score_sentence(feats, tags)\n",
    "        return forward_score - gold_score\n",
    "\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        # 通过BiLSTM提取发射分数\n",
    "        lstm_feats = self._get_lstm_features(sentence)\n",
    "        # 根据发射分数以及转移分数，通过viterbi解码找到一条最优路径\n",
    "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
    "        return score, tag_seq"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Model, self).__init__()\n",
    "        self.config = config\n",
    "        # if config.embedding_pretrained is not None:\n",
    "        #     self.embedding = nn.Embedding.from_pretrained(config.embedding_pretrained,\n",
    "        #                                                   freeze=False)  # 表示训练过程词嵌入向量会更新\n",
    "        # else:\n",
    "        self.embedding = nn.Embedding(self.config.vocab_len, self.config.embedding_dim,\n",
    "                                      padding_idx=self.configs.word2idx['<PAD>'])  # PAD索引填充\n",
    "\n",
    "        if self.config.bidirectional:\n",
    "            self.num_directions = 2\n",
    "        else:\n",
    "            self.num_directions = 1\n",
    "\n",
    "\n",
    "        self.rnn = nn.LSTM(input_size=self.config.embedding_dim,\n",
    "                           hidden_size=self.config.hidden_size,\n",
    "                           num_layers=self.config.num_layers,\n",
    "                           batch_first=True,\n",
    "                           bidirectional=self.config.bidirectional)\n",
    "\n",
    "        self.tag2idx = configs.tag2idx\n",
    "\n",
    "        # 转换参数矩阵 输入i,j是得分从j转换到i\n",
    "        self.tagset_size = len(self.tag2idx)\n",
    "        # 将lstm的输出映射到标记空间\n",
    "        self.hidden2tag = nn.Linear(self.config.hidden_size*self.num_directions, self.tagset_size)  # -> (B, num_class+2)  加上了START END\n",
    "        self.crf = CRF(num_tags=self.tagset_size,batch_first=True)\n",
    "\n",
    "    def _forward_alg(self, feats):\n",
    "        # 使用前向算法计算分区函数\n",
    "        init_alphas = self._make_tensor(torch.full((1, self.tagset_size), -10000.))\n",
    "        # START_TAG 包含所有得分\n",
    "        init_alphas[0][self.tag2idx[START_TAG]] = 0.\n",
    "\n",
    "        # 包装一个变量 以便获得自动反向提升\n",
    "        forward_var = init_alphas\n",
    "\n",
    "        # 通过句子迭代\n",
    "        for feat in feats:\n",
    "            alphas_t = []  # the forward tensor at this timestep\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # 广播发射得分：无论之前的标记是怎样的都是相同的\n",
    "                emit_score = feat[next_tag].view(1, -1).expand(1, self.tagset_size)\n",
    "                # trans_score 的第i个条目是从i转移到next_tag的分数\n",
    "                trans_score = self.transitions[next_tag].view(1, -1)\n",
    "                # next_tag_var 的第i个条目是执行log-sum-exp之前的变（i -> next_tag）的值\n",
    "                next_tag_var = forward_var + trans_score + emit_score\n",
    "                # 此标记的转发变量是所有分数的log-sum-exp\n",
    "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
    "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
    "        terminal_var = forward_var + self.transitions[self.tag2idx[STOP_TAG]]\n",
    "        alpha = log_sum_exp(terminal_var)\n",
    "        return alpha\n",
    "\n",
    "    def _score_sentence(self, feats, tags):\n",
    "        # Give the score of a provided tag sequence\n",
    "        score = self._make_tensor(torch.zeros(1))\n",
    "        tags = self._make_tensor(torch.cat([self._make_tensor(torch.tensor([self.tag2idx[START_TAG]], dtype=torch.long)),tags]))\n",
    "        for i, feat in enumerate(feats):\n",
    "            score = score + self.transitions[tags[i+1], tags[i]]+feat[tags[i+1]]\n",
    "        score = score + self.transitions[self.tag2idx[STOP_TAG], tags[-1]]\n",
    "        return score\n",
    "\n",
    "    def _viterbi_decode(self, feats):\n",
    "        backpointers = []\n",
    "\n",
    "        # Initialize the viterbi variables in log space\n",
    "        init_vvars = self._make_tensor(torch.full((1, self.tagset_size), -10000.))\n",
    "        init_vvars[0][self.tag2idx[START_TAG]] = 0\n",
    "\n",
    "        # forward_var at step o holds the viterbi variables for step i-1\n",
    "        forward_var = init_vvars\n",
    "        for feat in feats:\n",
    "            bptrs_t = []  # hold the backpointers for this step\n",
    "            viterbivars_t = []  # holds the viterbi variables for this step\n",
    "\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # next_tag_var[i]保存上一步的标签i的viterbi变量\n",
    "                # 加上标签i转换到next_tag的分数 我们这里不包括emission分数 因为最大值不依赖于它们（在下面添加它们）\n",
    "                next_tag_var = forward_var + self.transitions[next_tag]\n",
    "                best_tag_id = argmax(next_tag_var)\n",
    "                bptrs_t.append(best_tag_id)\n",
    "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
    "            # 现在添加emission分数 并将forward_var分配给刚计算的viterbi变量集\n",
    "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
    "            backpointers.append(bptrs_t)\n",
    "\n",
    "        # 过渡到STOP_TAG\n",
    "        terminal_var = forward_var + self.transitions[self.tag2idx[STOP_TAG]]\n",
    "        best_tag_id = argmax(terminal_var)\n",
    "        path_score = terminal_var[0][best_tag_id]\n",
    "\n",
    "        # 按照后退指针解码最佳路径\n",
    "        best_path = [best_tag_id]\n",
    "        for bptrs_t in reversed(backpointers):\n",
    "            best_tag_id = bptrs_t[best_tag_id]\n",
    "            best_path.append(best_tag_id)\n",
    "        # 弹出开始标记（我们不想将器返回给调用者）\n",
    "        start = best_path.pop()\n",
    "        assert start == self.tag2idx[START_TAG]  # Sanity check\n",
    "        best_path.reverse()\n",
    "        return path_score, best_path\n",
    "\n",
    "    def _get_lstm_features(self, x):\n",
    "        # 数据预处理时，x被处理成是一个tuple,其内容是: (word, label).\n",
    "        # x:b_size\n",
    "        x = self.embedding(x)  # B -> (B, e_d)\n",
    "        x = x.unsqueeze(1)  # (B, e_b) -> (B, 1, e_b)\n",
    "        h_0, c_0 = self._init_hidden(batchs=x.size(0))\n",
    "        out, (hidden, c) = self.rnn(x,(h_0, c_0))  # out:(B, 1, num_directions*hidden_size) hidden:(num_layer*nun_directions, B,  hidden_size)\n",
    "        # out = out.squeeze(1)\n",
    "        # output is batch_first but hidden not\n",
    "        out = self.hidden2tag(out)  # (B,num_directions*hidden_size) -> (B, num_class)\n",
    "        out = out.transpose(0, 1)\n",
    "        return out\n",
    "\n",
    "    def neg_log_likelihood(self, x, tags):  # 损失函数\n",
    "        tags = tags.unsqueeze(0)\n",
    "        feats = self._get_lstm_features(x)\n",
    "        return -self.crf(feats, tags)\n",
    "\n",
    "\n",
    "\n",
    "    def _init_hidden(self, batchs):  # 初始化h_0和c_0 与GRU不同的是多了c_0（细胞状态）\n",
    "        h_0 = torch.zeros(self.config.num_layers*self.num_directions, batchs,  self.config.hidden_size)\n",
    "        c_0 = torch.zeros(self.config.num_layers*self.num_directions, batchs, self.config.hidden_size)\n",
    "        return self._make_tensor(h_0), self._make_tensor(c_0)\n",
    "\n",
    "    def _make_tensor(self, tensor):\n",
    "        # 函数说明： 将传入的tensor转移到cpu或gpu内\n",
    "\n",
    "        tensor_ret = tensor.to(self.config.device)\n",
    "        return tensor_ret\n",
    "\n",
    "    # def getTagLs(self, config):\n",
    "    #     tag_ls = config.class_ls\n",
    "    #     tag_ls.append(\"<START>\")\n",
    "    #     tag_ls.append(\"<STOP>\")\n",
    "    #     return tag_ls\n",
    "    #\n",
    "    # def getTagDic(self):\n",
    "    #     tag_dic = {}\n",
    "    #     for idx, label in enumerate(self.tag_ls):\n",
    "    #         tag_dic[label] = idx\n",
    "    #     return tag_dic\n",
    "    #\n",
    "    # def idx2Tag(self, idx):\n",
    "    #     return self.tag_ls[idx]\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 数据预处理时，x被处理成是一个tuple,其内容是: (word, label).\n",
    "        # x:b_size\n",
    "        lstm_feats = self._get_lstm_features(x)  # 获取BiLSTM的emission分数\n",
    "\n",
    "        out = self.crf.decode(lstm_feats)\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "from utils.param_configs import Configs\n",
    "configs = Configs()\n",
    "\n",
    "# 将训练集汉字使用数字表示\n",
    "# 为了方便调试，先用100条数据进行训练，调试好后可用全量数据进行训练\n",
    "training_data = training_data_txt[:]\n",
    "# --------------------------建立字典，字: idx-------------------------------------\n",
    "word2idx = {}\n",
    "# 训练集的\n",
    "for sentence, tags in training_data:\n",
    "    for word in sentence:\n",
    "        if word not in word2idx:\n",
    "            word2idx[word] = len(word2idx)\n",
    "# 测试集的\n",
    "testing_data = testing_data_txt\n",
    "for sentence in testing_data:\n",
    "    for word in sentence:\n",
    "        if word not in word2idx:\n",
    "            word2idx[word] = len(word2idx)\n",
    "\n",
    "# 加2个特殊字符\n",
    "word2idx['<UNK>'] = len(word2idx)\n",
    "word2idx['<PAD>'] = len(word2idx)\n",
    "\n",
    "configs.word2idx = word2idx\n",
    "# ------------------------------------------------------------------------------"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "10000"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data_txt[0][0]),len(training_data_txt[0][1])\n",
    "len(training_data_txt)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "from utils.data_process import create_data_loader\n",
    "train_data_loader = create_data_loader(training_data_txt, configs)\n",
    "# test_data_loader = create_data_loader(testing_data_txt, configs) # 没有标签的测试集就不这样构建，因为没有label"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# testing_data_txt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence': ['交 行 1 4 年 用 过 ， 半 年 准 备 提 额 ， 却 直 接 被 降 到 1 Ｋ ， 半 年 期 间 只 T 过 一 次 三 千 ， 其 它 全 部 真 实 消 费 ， 第 六 个 月 的 时 候 为 了 增 加 评 分 提 额 ， 还 特 意 分 期 两 万 ， 但 降 额 后 电 话 投 诉 ， 申 请 提 . . . <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>', '单 标 我 有 了 ， 最 近 v i s a 双 标 返 现 活 动 好 <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>', '建 设 银 行 提 额 很 慢 的 … … <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>', '孙 女 士 在 原 恒 泰 农 村 合 作 银 行 存 入 5 0 万 元 ， 同 年 9 月 又 存 款 5 0 万 元 。 2 0 1 4 年 ， 恒 泰 农 村 合 作 银 行 进 行 改 制 ， 改 制 后 的 银 行 便 是 枣 庄 农 商 银 行 。 <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>', '我 的 怎 么 显 示 0 . 2 5 费 率 ， 而 且 不 管 分 多 少 期 都 一 样 费 率 ， 可 惜 只 有 6 9 k <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>', '利 率 不 错 ， 可 以 撸 <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>', '不 能 ? ? 好 像 房 贷 跟 信 用 卡 是 分 开 审 核 的 反 正 我 的 不 得 <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>', '我 感 觉 这 样 才 是 合 理 的 ， 花 呗 白 条 没 要 那 么 多 信 息 ， 照 样 可 以 给 额 度 。 有 征 信 威 慑 ， 没 那 么 多 人 敢 借 了 不 还 。 与 其 眉 毛 胡 子 一 把 抓 ， 还 不 如 按 额 度 区 间 对 客 户 进 行 不 同 程 度 的 调 查 ， 免 . . . <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>', '羡 慕 ， 可 能 上 个 月 申 请 多 了 。 上 月 连 续 下 了 浦 发 广 发 华 夏 交 通 。 这 个 月 申 请 建 行 ， 农 业 ? ? 邮 储 各 种 秒 拒 ， 买 了 点 建 行 理 财 ， 3 个 月 后 在 看 . . . <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>', '这 个 短 债 只 是 用 来 提 升 信 誉 刷 建 行 预 审 批 的 ， 又 不 是 什 么 赚 钱 的 基 金 。 也 只 有 建 行 有 卖 ， 我 买 过 ， 不 但 不 赚 钱 还 会 亏 本 的 。 . . . <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>', '打 电 话 问 中 信 信 用 卡 为 啥 没 批 呗 <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>', '大 部 分 是 凉 的 <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>', '这 几 天 也 接 到 了 建 行 的 分 期 电 话 ， 让 我 1 . 6 分 1 2 期 ， 跟 他 说 5 0 0 0 ， 3 期 ， 意 思 意 思 。 快 两 年 了 还 没 首 提 ， 心 塞 <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>', '我 是 建 行 贷 款 ， 别 的 银 行 信 用 卡 没 还 ， 账 单 大 概 4 w <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>', '微 粒 贷 ， 借 呗 ， 这 些 也 是 多 头 的 因 素 。 因 为 这 些 都 上 征 信 。 我 两 行 卡 ， 都 是 多 头 。 因 为 我 也 是 有 使 用 微 粒 贷 ， 花 呗 ， 借 呗 ， 白 条 。 查 征 信 显 示 这 些 全 部 上 征 信 了 。 . . . <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>', '别 急 。 说 不 准 的 。 第 一 次 过 的 时 候 也 审 核 了 十 几 天 。 不 过 最 后 全 额 度 通 过 。 利 息 高 就 没 用 <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>'], 'label': ['B-BANK I-BANK O O O O O O O O O O B-COMMENTS_N I-COMMENTS_N O O O O O B-COMMENTS_ADJ I-COMMENTS_ADJ O O O O O O O O O O O O O O O O O O O O O B-COMMENTS_N I-COMMENTS_N O O O O O O O O O O B-COMMENTS_N I-COMMENTS_N O O B-COMMENTS_N I-COMMENTS_N O O O O B-PRODUCT I-PRODUCT O O O O B-COMMENTS_ADJ O O O O O O O O O O O O O O O O O O O O O O O O O O O O O', 'B-PRODUCT I-PRODUCT O O O O O O B-PRODUCT I-PRODUCT I-PRODUCT I-PRODUCT B-PRODUCT I-PRODUCT B-COMMENTS_N I-COMMENTS_N I-COMMENTS_N I-COMMENTS_N B-COMMENTS_ADJ O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O', 'B-BANK I-BANK I-BANK I-BANK B-COMMENTS_N I-COMMENTS_N B-COMMENTS_ADJ I-COMMENTS_ADJ O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O', 'O O O O O B-BANK I-BANK I-BANK I-BANK I-BANK I-BANK I-BANK I-BANK O O O O O O O O O O O O B-PRODUCT I-PRODUCT O O O O O O O O O O O B-BANK I-BANK I-BANK I-BANK I-BANK I-BANK I-BANK I-BANK O O O O O O O O O O O O O B-BANK I-BANK I-BANK I-BANK I-BANK I-BANK O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O', 'O O O O O O O O O O B-COMMENTS_N I-COMMENTS_N O O O O O O B-COMMENTS_ADJ I-COMMENTS_ADJ O O O O B-COMMENTS_N I-COMMENTS_N O B-COMMENTS_ADJ I-COMMENTS_ADJ B-COMMENTS_ADJ I-COMMENTS_ADJ O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O', 'B-COMMENTS_N I-COMMENTS_N B-COMMENTS_ADJ I-COMMENTS_ADJ O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O', 'B-COMMENTS_ADJ I-COMMENTS_ADJ O O O O B-PRODUCT I-PRODUCT O B-PRODUCT I-PRODUCT I-PRODUCT O O O B-COMMENTS_N I-COMMENTS_N O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O', 'O O O O O O O B-COMMENTS_ADJ I-COMMENTS_ADJ O O B-PRODUCT I-PRODUCT B-PRODUCT I-PRODUCT O O O O O O O O O O O O O B-COMMENTS_N I-COMMENTS_N O O B-PRODUCT I-PRODUCT O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-COMMENTS_N I-COMMENTS_N O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O', 'B-COMMENTS_ADJ I-COMMENTS_ADJ O O O O O O O O O O O O O O O O O B-BANK I-BANK B-BANK I-BANK B-BANK I-BANK B-BANK I-BANK O O O O O O B-BANK I-BANK O B-BANK I-BANK O O B-BANK I-BANK O O O B-COMMENTS_ADJ O O O O B-BANK I-BANK B-PRODUCT I-PRODUCT O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O', 'O O B-PRODUCT I-PRODUCT O O O O B-COMMENTS_ADJ I-COMMENTS_ADJ O O O B-BANK I-BANK B-COMMENTS_N I-COMMENTS_N I-COMMENTS_N O O O O O O O B-COMMENTS_ADJ I-COMMENTS_ADJ O B-PRODUCT I-PRODUCT O O B-COMMENTS_ADJ I-COMMENTS_ADJ B-BANK I-BANK O O O O O O O O O B-COMMENTS_ADJ I-COMMENTS_ADJ I-COMMENTS_ADJ O O B-COMMENTS_ADJ I-COMMENTS_ADJ O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O', 'O O O O O O B-PRODUCT I-PRODUCT I-PRODUCT O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O', 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O', 'O O O O O O O B-BANK I-BANK O B-PRODUCT I-PRODUCT O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-COMMENTS_N I-COMMENTS_N O B-COMMENTS_ADJ I-COMMENTS_ADJ O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O', 'O O B-BANK I-BANK B-PRODUCT I-PRODUCT O O O O O B-PRODUCT I-PRODUCT I-PRODUCT O O O B-COMMENTS_N I-COMMENTS_N O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O', 'B-PRODUCT I-PRODUCT I-PRODUCT O B-PRODUCT I-PRODUCT O O O O O O O O O O O O O O O O O B-PRODUCT I-PRODUCT O O O O O O O O O O O O O O O O O O O B-PRODUCT I-PRODUCT I-PRODUCT O B-PRODUCT I-PRODUCT O B-PRODUCT I-PRODUCT O B-PRODUCT I-PRODUCT O O B-PRODUCT I-PRODUCT O O O O O O O B-PRODUCT I-PRODUCT O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O', 'B-COMMENTS_ADJ I-COMMENTS_ADJ O O B-COMMENTS_ADJ I-COMMENTS_ADJ O O O O O O O O O O B-COMMENTS_N I-COMMENTS_N O O O O O O O O O B-COMMENTS_N I-COMMENTS_N O O O O B-COMMENTS_N I-COMMENTS_N B-COMMENTS_ADJ O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O'], 'sentence_tensor': tensor([[   0,    1,    2,  ..., 2621, 2621, 2621],\n",
      "        [  63,   64,   65,  ..., 2621, 2621, 2621],\n",
      "        [  79,   80,   81,  ..., 2621, 2621, 2621],\n",
      "        ...,\n",
      "        [  65,  111,   79,  ..., 2621, 2621, 2621],\n",
      "        [ 251,  252,  140,  ..., 2621, 2621, 2621],\n",
      "        [ 247,  258,  105,  ..., 2621, 2621, 2621]]), 'label_tensor': tensor([[1, 2, 0,  ..., 0, 0, 0],\n",
      "        [3, 4, 0,  ..., 0, 0, 0],\n",
      "        [1, 2, 2,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 1,  ..., 0, 0, 0],\n",
      "        [3, 4, 4,  ..., 0, 0, 0],\n",
      "        [7, 8, 0,  ..., 0, 0, 0]]), 'mask_tensor': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "for sample in train_data_loader:\n",
    "    print(sample)\n",
    "    break\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "625"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "configs.word2idx['交']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "['交 行 1 4 年 用 过 ， 半 年 准 备 提 额 ， 却 直 接 被 降 到 1 Ｋ ， 半 年 期 间 只 T 过 一 次 三 千 ， 其 它 全 部 真 实 消 费 ， 第 六 个 月 的 时 候 为 了 增 加 评 分 提 额 ， 还 特 意 分 期 两 万 ， 但 降 额 后 电 话 投 诉 ， 申 请 提 . . . <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>',\n '单 标 我 有 了 ， 最 近 v i s a 双 标 返 现 活 动 好 <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>',\n '建 设 银 行 提 额 很 慢 的 … … <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>',\n '孙 女 士 在 原 恒 泰 农 村 合 作 银 行 存 入 5 0 万 元 ， 同 年 9 月 又 存 款 5 0 万 元 。 2 0 1 4 年 ， 恒 泰 农 村 合 作 银 行 进 行 改 制 ， 改 制 后 的 银 行 便 是 枣 庄 农 商 银 行 。 <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>',\n '我 的 怎 么 显 示 0 . 2 5 费 率 ， 而 且 不 管 分 多 少 期 都 一 样 费 率 ， 可 惜 只 有 6 9 k <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>',\n '利 率 不 错 ， 可 以 撸 <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>',\n '不 能 ? ? 好 像 房 贷 跟 信 用 卡 是 分 开 审 核 的 反 正 我 的 不 得 <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>',\n '我 感 觉 这 样 才 是 合 理 的 ， 花 呗 白 条 没 要 那 么 多 信 息 ， 照 样 可 以 给 额 度 。 有 征 信 威 慑 ， 没 那 么 多 人 敢 借 了 不 还 。 与 其 眉 毛 胡 子 一 把 抓 ， 还 不 如 按 额 度 区 间 对 客 户 进 行 不 同 程 度 的 调 查 ， 免 . . . <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>',\n '羡 慕 ， 可 能 上 个 月 申 请 多 了 。 上 月 连 续 下 了 浦 发 广 发 华 夏 交 通 。 这 个 月 申 请 建 行 ， 农 业 ? ? 邮 储 各 种 秒 拒 ， 买 了 点 建 行 理 财 ， 3 个 月 后 在 看 . . . <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>',\n '这 个 短 债 只 是 用 来 提 升 信 誉 刷 建 行 预 审 批 的 ， 又 不 是 什 么 赚 钱 的 基 金 。 也 只 有 建 行 有 卖 ， 我 买 过 ， 不 但 不 赚 钱 还 会 亏 本 的 。 . . . <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>',\n '打 电 话 问 中 信 信 用 卡 为 啥 没 批 呗 <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>',\n '大 部 分 是 凉 的 <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>',\n '这 几 天 也 接 到 了 建 行 的 分 期 电 话 ， 让 我 1 . 6 分 1 2 期 ， 跟 他 说 5 0 0 0 ， 3 期 ， 意 思 意 思 。 快 两 年 了 还 没 首 提 ， 心 塞 <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>',\n '我 是 建 行 贷 款 ， 别 的 银 行 信 用 卡 没 还 ， 账 单 大 概 4 w <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>',\n '微 粒 贷 ， 借 呗 ， 这 些 也 是 多 头 的 因 素 。 因 为 这 些 都 上 征 信 。 我 两 行 卡 ， 都 是 多 头 。 因 为 我 也 是 有 使 用 微 粒 贷 ， 花 呗 ， 借 呗 ， 白 条 。 查 征 信 显 示 这 些 全 部 上 征 信 了 。 . . . <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>',\n '别 急 。 说 不 准 的 。 第 一 次 过 的 时 候 也 审 核 了 十 几 天 。 不 过 最 后 全 额 度 通 过 。 利 息 高 就 没 用 <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>']"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['sentence']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "['B-BANK I-BANK O O O O O O O O O O B-COMMENTS_N I-COMMENTS_N O O O O O B-COMMENTS_ADJ I-COMMENTS_ADJ O O O O O O O O O O O O O O O O O O O O O B-COMMENTS_N I-COMMENTS_N O O O O O O O O O O B-COMMENTS_N I-COMMENTS_N O O B-COMMENTS_N I-COMMENTS_N O O O O B-PRODUCT I-PRODUCT O O O O B-COMMENTS_ADJ O O O O O O O O O O O O O O O O O O O O O O O O O O O O O',\n 'B-PRODUCT I-PRODUCT O O O O O O B-PRODUCT I-PRODUCT I-PRODUCT I-PRODUCT B-PRODUCT I-PRODUCT B-COMMENTS_N I-COMMENTS_N I-COMMENTS_N I-COMMENTS_N B-COMMENTS_ADJ O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O',\n 'B-BANK I-BANK I-BANK I-BANK B-COMMENTS_N I-COMMENTS_N B-COMMENTS_ADJ I-COMMENTS_ADJ O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O',\n 'O O O O O B-BANK I-BANK I-BANK I-BANK I-BANK I-BANK I-BANK I-BANK O O O O O O O O O O O O B-PRODUCT I-PRODUCT O O O O O O O O O O O B-BANK I-BANK I-BANK I-BANK I-BANK I-BANK I-BANK I-BANK O O O O O O O O O O O O O B-BANK I-BANK I-BANK I-BANK I-BANK I-BANK O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O',\n 'O O O O O O O O O O B-COMMENTS_N I-COMMENTS_N O O O O O O B-COMMENTS_ADJ I-COMMENTS_ADJ O O O O B-COMMENTS_N I-COMMENTS_N O B-COMMENTS_ADJ I-COMMENTS_ADJ B-COMMENTS_ADJ I-COMMENTS_ADJ O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O',\n 'B-COMMENTS_N I-COMMENTS_N B-COMMENTS_ADJ I-COMMENTS_ADJ O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O',\n 'B-COMMENTS_ADJ I-COMMENTS_ADJ O O O O B-PRODUCT I-PRODUCT O B-PRODUCT I-PRODUCT I-PRODUCT O O O B-COMMENTS_N I-COMMENTS_N O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O',\n 'O O O O O O O B-COMMENTS_ADJ I-COMMENTS_ADJ O O B-PRODUCT I-PRODUCT B-PRODUCT I-PRODUCT O O O O O O O O O O O O O B-COMMENTS_N I-COMMENTS_N O O B-PRODUCT I-PRODUCT O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-COMMENTS_N I-COMMENTS_N O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O',\n 'B-COMMENTS_ADJ I-COMMENTS_ADJ O O O O O O O O O O O O O O O O O B-BANK I-BANK B-BANK I-BANK B-BANK I-BANK B-BANK I-BANK O O O O O O B-BANK I-BANK O B-BANK I-BANK O O B-BANK I-BANK O O O B-COMMENTS_ADJ O O O O B-BANK I-BANK B-PRODUCT I-PRODUCT O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O',\n 'O O B-PRODUCT I-PRODUCT O O O O B-COMMENTS_ADJ I-COMMENTS_ADJ O O O B-BANK I-BANK B-COMMENTS_N I-COMMENTS_N I-COMMENTS_N O O O O O O O B-COMMENTS_ADJ I-COMMENTS_ADJ O B-PRODUCT I-PRODUCT O O B-COMMENTS_ADJ I-COMMENTS_ADJ B-BANK I-BANK O O O O O O O O O B-COMMENTS_ADJ I-COMMENTS_ADJ I-COMMENTS_ADJ O O B-COMMENTS_ADJ I-COMMENTS_ADJ O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O',\n 'O O O O O O B-PRODUCT I-PRODUCT I-PRODUCT O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O',\n 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O',\n 'O O O O O O O B-BANK I-BANK O B-PRODUCT I-PRODUCT O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-COMMENTS_N I-COMMENTS_N O B-COMMENTS_ADJ I-COMMENTS_ADJ O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O',\n 'O O B-BANK I-BANK B-PRODUCT I-PRODUCT O O O O O B-PRODUCT I-PRODUCT I-PRODUCT O O O B-COMMENTS_N I-COMMENTS_N O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O',\n 'B-PRODUCT I-PRODUCT I-PRODUCT O B-PRODUCT I-PRODUCT O O O O O O O O O O O O O O O O O B-PRODUCT I-PRODUCT O O O O O O O O O O O O O O O O O O O B-PRODUCT I-PRODUCT I-PRODUCT O B-PRODUCT I-PRODUCT O B-PRODUCT I-PRODUCT O B-PRODUCT I-PRODUCT O O B-PRODUCT I-PRODUCT O O O O O O O B-PRODUCT I-PRODUCT O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O',\n 'B-COMMENTS_ADJ I-COMMENTS_ADJ O O B-COMMENTS_ADJ I-COMMENTS_ADJ O O O O O O O O O O B-COMMENTS_N I-COMMENTS_N O O O O O O O O O B-COMMENTS_N I-COMMENTS_N O O O O B-COMMENTS_N I-COMMENTS_N B-COMMENTS_ADJ O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O']"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['label']\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[   0,    1,    2,  ..., 2621, 2621, 2621],\n        [  63,   64,   65,  ..., 2621, 2621, 2621],\n        [  79,   80,   81,  ..., 2621, 2621, 2621],\n        ...,\n        [  65,  111,   79,  ..., 2621, 2621, 2621],\n        [ 251,  252,  140,  ..., 2621, 2621, 2621],\n        [ 247,  258,  105,  ..., 2621, 2621, 2621]])"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['sentence_tensor']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([100])"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['mask_tensor'][0].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1, 2, 0,  ..., 0, 0, 0],\n        [3, 4, 0,  ..., 0, 0, 0],\n        [1, 2, 2,  ..., 0, 0, 0],\n        ...,\n        [0, 0, 1,  ..., 0, 0, 0],\n        [3, 4, 4,  ..., 0, 0, 0],\n        [7, 8, 0,  ..., 0, 0, 0]])"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['label_tensor']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "START_TAG = \"<START>\"\n",
    "STOP_TAG = \"<STOP>\"\n",
    "EMBEDDING_DIM = 11\n",
    "HIDDEN_DIM = 6\n",
    "\n",
    "\n",
    "# 将训练集汉字使用数字表示\n",
    "# 为了方便调试，先用100条数据进行训练，调试好后可用全量数据进行训练\n",
    "training_data = training_data_txt[:]\n",
    "\n",
    "# --------------------------建立字典，字: idx-------------------------------------\n",
    "word2idx = {}\n",
    "# 训练集的\n",
    "for sentence, tags in training_data:\n",
    "    for word in sentence:\n",
    "        if word not in word2idx:\n",
    "            word2idx[word] = len(word2idx)\n",
    "# 测试集的\n",
    "testing_data = testing_data_txt\n",
    "for sentence in testing_data:\n",
    "    for word in sentence:\n",
    "        if word not in word2idx:\n",
    "            word2idx[word] = len(word2idx)\n",
    "# ------------------------------------------------------------------------------\n",
    "START_TAG = \"<START>\"\n",
    "STOP_TAG = \"<STOP>\"\n",
    "# 标签：idx\n",
    "tag2idx = { \"O\": 0, \"B-BANK\": 1, \"I-BANK\": 2, \"B-PRODUCT\":3,'I-PRODUCT':4,\n",
    "             'B-COMMENTS_N':5, 'I-COMMENTS_N':6, 'B-COMMENTS_ADJ':7,\n",
    "             'I-COMMENTS_ADJ':8, START_TAG: 9, STOP_TAG: 10}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = BiLSTM_CRF(len(word2idx), tag2idx, EMBEDDING_DIM, HIDDEN_DIM)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 检查下模型输入输出\n",
    "precheck_sent = prepare_sequence(training_data[0][0], word2idx)\n",
    "precheck_tags = torch.tensor([tag2idx[t] for t in training_data[0][1]], dtype=torch.long)\n",
    "print(model(precheck_sent))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import time\n",
    "t = time.time()\n",
    "# Make sure prepare_sequence from earlier in the LSTM section is loaded\n",
    "for epoch in range(100):\n",
    "    print('the',epoch,' epoch')\n",
    "    print(f'Time Taken: {round(time.time()-t)} seconds')\n",
    "    for sentence, tags in training_data:\n",
    "        # 第一步，pytorch梯度累积，需要清零梯度\n",
    "        model.zero_grad()\n",
    "\n",
    "        # 第二步，将输入转化为tensors\n",
    "        sentence_in = prepare_sequence(sentence, word2idx)\n",
    "        targets = torch.tensor([tag2idx[t] for t in tags], dtype=torch.long)\n",
    "\n",
    "        # 进行前向计算，取出crf loss\n",
    "        loss = model.neg_log_likelihood(sentence_in, targets)\n",
    "\n",
    "        # 第四步，计算loss，梯度，通过optimier更新参数\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 训练结束查看模型预测结果，对比观察模型是否学到\n",
    "# 标签：idx\n",
    "idx2tag = { 0:\"O\", 1:\"B-BANK\", 2:\"I-BANK\", 3:\"B-PRODUCT\",4:'I-PRODUCT',\n",
    "             5:'B-COMMENTS_N', 6:'I-COMMENTS_N', 7:'B-COMMENTS_ADJ',\n",
    "             8:'I-COMMENTS_ADJ', 9:START_TAG, 10:STOP_TAG}\n",
    "with torch.no_grad():\n",
    "    precheck_sent = prepare_sequence(training_data[4][0], word2idx)\n",
    "    print(model(precheck_sent))\n",
    "    a = model(precheck_sent) # model return score, tag_seq\n",
    "    # a = pd.Series(a)\n",
    "    print('句子为：', ''.join(training_data[4][0]))\n",
    "    print('实体标注结果为：', ' '.join([idx2tag[i] for i in a[1]]))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}