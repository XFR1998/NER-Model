{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# bi-lstm+crf，使用pytorch-crf库实现crf，可cuda加速。\n",
    "\n",
    "数据集说明：\n",
    "\n",
    "1: B-BANK 代表银行实体的开始\n",
    "\n",
    "2: I-BANK 代表银行实体的内部\n",
    "\n",
    "3: B-PRODUCT 代表产品实体的开始\n",
    "\n",
    "4: I-PRODUCT 代表产品实体的内部\n",
    "\n",
    "5: O 代表不属于标注的范围\n",
    "\n",
    "6: B-COMMENTS_N 代表用户评论（名词）\n",
    "\n",
    "7: I-COMMENTS_N 代表用户评论（名词）实体的内部\n",
    "\n",
    "8: B-COMMENTS_ADJ 代表用户评论（形容词）\n",
    "\n",
    "9: I-COMMENTS_ADJ 代表用户评论（形容词）实体的内部"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda能用\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchcrf import CRF\n",
    "torch.manual_seed(1)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('{}能用'.format(device))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./train_data_public.csv')\n",
    "train_data.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "test_data = pd.read_csv('./test_public.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   text        10000 non-null  object\n",
      " 1   BIO_anno    10000 non-null  object\n",
      " 2   class       10000 non-null  int64 \n",
      " 3   bank_topic  7636 non-null   object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 312.6+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5093 entries, 0 to 5092\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      5093 non-null   int64 \n",
      " 1   text    5093 non-null   object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 79.7+ KB\n"
     ]
    }
   ],
   "source": [
    "train_data.info()\n",
    "test_data.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                text  \\\n0  交行14年用过，半年准备提额，却直接被降到1Ｋ，半年期间只T过一次三千，其它全部真实消费，第...   \n1                                单标我有了，最近visa双标返现活动好   \n2                                        建设银行提额很慢的……   \n\n                                            BIO_anno  class bank_topic  \n0  B-BANK I-BANK O O O O O O O O O O B-COMMENTS_N...      0       建设银行  \n1  B-PRODUCT I-PRODUCT O O O O O O B-PRODUCT I-PR...      1       建设银行  \n2  B-BANK I-BANK I-BANK I-BANK B-COMMENTS_N I-COM...      0       建设银行  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>BIO_anno</th>\n      <th>class</th>\n      <th>bank_topic</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>交行14年用过，半年准备提额，却直接被降到1Ｋ，半年期间只T过一次三千，其它全部真实消费，第...</td>\n      <td>B-BANK I-BANK O O O O O O O O O O B-COMMENTS_N...</td>\n      <td>0</td>\n      <td>建设银行</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>单标我有了，最近visa双标返现活动好</td>\n      <td>B-PRODUCT I-PRODUCT O O O O O O B-PRODUCT I-PR...</td>\n      <td>1</td>\n      <td>建设银行</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>建设银行提额很慢的……</td>\n      <td>B-BANK I-BANK I-BANK I-BANK B-COMMENTS_N I-COM...</td>\n      <td>0</td>\n      <td>建设银行</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# 把text和标注按单个字分隔开，放进列表\n",
    "train_data['BIO_anno'] = train_data['BIO_anno'].apply(lambda x:x.split(' '))\n",
    "# 将text和标注组合存进元组\n",
    "train_data['training_data'] = train_data.apply(lambda row: (list(row['text']),row['BIO_anno']), axis=1)\n",
    "test_data['testing_data'] = test_data.apply(lambda row: list(row['text']), axis=1)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# train_data['training_data'][0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集大小： 10000 测试集大小： 5093\n"
     ]
    }
   ],
   "source": [
    "training_data_txt = train_data['training_data'].to_list()\n",
    "testing_data_txt = test_data['testing_data'].to_list()\n",
    "print('训练集大小：',len(training_data_txt), '测试集大小：',len(testing_data_txt))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# 定义一些工具函数\n",
    "\n",
    "# 句子转idx\n",
    "def prepare_sequence(seq, word2idx):\n",
    "    idxs = [word2idx[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "def argmax(vec):\n",
    "    # return the argmax as a python int\n",
    "    # 返回vec的dim为1维度上的最大值索引\n",
    "    _, idx = torch.max(vec,axis=1)\n",
    "    return idx.item()\n",
    "\n",
    "# Compute log sum exp in a numerically stable way for the forward algorithm\n",
    "# 前向算法是不断累积之前的结果，这样就会有个缺点\n",
    "# 指数和累积到一定程度后，会超过计算机浮点值的最大值，变成inf，这样取log后也是inf\n",
    "# 为了避免这种情况，用一个合适的值clip去提指数和的公因子，这样就不会使某项变得过大而无法计算\n",
    "# SUM = log(exp(s1)+exp(s2)+...+exp(s100))\n",
    "#     = log{exp(clip)*[exp(s1-clip)+exp(s2-clip)+...+exp(s100-clip)]}\n",
    "#     = clip + log[exp(s1-clip)+exp(s2-clip)+...+exp(s100-clip)]\n",
    "# where clip=max\n",
    "def log_sum_exp(vec):\n",
    "    max_score = vec[0, argmax(vec)]\n",
    "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
    "    return max_score + torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# 定义网络结构：bi-lstm + crf\n",
    "class BiLSTM_CRF(nn.Module):\n",
    "    def __init__(self, vocab_size, tag2idx, embedding_dim, hidden_dim):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tag2idx = tag2idx\n",
    "        self.tagset_size = len(tag2idx)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeds = nn.Embedding(vocab_size, self.embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim,\n",
    "                            hidden_size=self.hidden_dim//2, # 双向lstm，最后拼接后就是hidden_dim了。\n",
    "                            num_layers=1,\n",
    "                            bidirectional=True)\n",
    "        # 将BiLSTM提取的特征向量映射到特征空间，即经过全连接得到发射分数\n",
    "        self.hidden2tag = nn.Linear(self.hidden_dim, self.tagset_size)\n",
    "\n",
    "        # 转移矩阵的参数初始化，transitions[i,j]代表的是从第j个tag转移到第i个tag的转移分数\n",
    "        self.transitions = nn.Parameter(torch.randn(self.tagset_size, self.tagset_size))\n",
    "\n",
    "        # 初始化所有其他tag转移到START_TAG的分数非常小，即不可能由其他tag转移到START_TAG\n",
    "        # 初始化STOP_TAG转移到所有其他tag的分数非常小，即不可能由STOP_TAG转移到其他tag\n",
    "        self.transitions.data[tag2idx[START_TAG], :] = -10000\n",
    "        self.transitions.data[:, tag2idx[STOP_TAG]] = -10000\n",
    "\n",
    "        self.hidden = self.init_hidden()\n",
    "    def init_hidden(self):\n",
    "        # 初始化lstm参数\n",
    "        # h0, c0的shape: (num_layers*2, bs, hidden_size), 双向就乘2\n",
    "        h0 = torch.randn(2, 1, self.hidden_dim//2)\n",
    "        c0 = torch.randn(2, 1, self.hidden_dim//2)\n",
    "        return (h0, c0)\n",
    "\n",
    "    def _get_lstm_features(self, sentence):\n",
    "        # 通过bi-lstm提取特征\n",
    "        self.hidden = self.init_hidden()\n",
    "        embeds = self.word_embeds(sentence).view(len(sentence),1,-1)\n",
    "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
    "        print(lstm_out.shape)\n",
    "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
    "        print(lstm_out.shape)\n",
    "        lstm_feats = self.hidden2tag(lstm_out)\n",
    "        print(lstm_feats.shape)\n",
    "        return lstm_feats\n",
    "\n",
    "    def _score_sentence(self, feats, tags):\n",
    "        # 计算给定tag序列的分数，即一条路径的分数\n",
    "        score = torch.zeros(1)\n",
    "        tags = torch.cat([torch.tensor([self.tag2idx[START_TAG]], dtype=torch.long), tags])\n",
    "        for i, feat in enumerate(feats):\n",
    "            # 递推计算路径分数：转移分数 + 发射分数\n",
    "            score = score + self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
    "        score = score + self.transitions[self.tag2idx[STOP_TAG], tags[-1]]\n",
    "        return score\n",
    "\n",
    "    def _forward_alg(self, feats):\n",
    "        # 通过前向算法递推计算\n",
    "        init_alphas = torch.full((1, self.tagset_size), -10000.)\n",
    "        # 初始化step 0即START位置的发射分数，START_TAG取0其他位置取-10000\n",
    "        init_alphas[0][self.tag2idx[START_TAG]] = 0.\n",
    "\n",
    "        # 将初始化START位置为0的发射分数赋值给previous\n",
    "        previous = init_alphas\n",
    "\n",
    "        # 迭代整个句子\n",
    "        for obs in feats:\n",
    "            # 当前时间步的前向tensor\n",
    "            alphas_t = []\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # 取出当前tag的发射分数，与之前时间步的tag无关\n",
    "                emit_score = obs[next_tag].view(1, -1).expand(1, self.tagset_size)\n",
    "                # 取出当前tag由之前tag转移过来的转移分数\n",
    "                trans_score = self.transitions[next_tag].view(1, -1)\n",
    "                # 当前路径的分数：之前时间步分数 + 转移分数 + 发射分数\n",
    "                next_tag_var = previous + trans_score + emit_score\n",
    "                # 对当前分数取log-sum-exp\n",
    "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
    "            # 更新previous 递推计算下一个时间步\n",
    "            previous = torch.cat(alphas_t).view(1, -1)\n",
    "        # 考虑最终转移到STOP_TAG\n",
    "        terminal_var = previous + self.transitions[self.tag2idx[STOP_TAG]]\n",
    "        # 计算最终的分数\n",
    "        scores = log_sum_exp(terminal_var)\n",
    "        return scores\n",
    "\n",
    "\n",
    "    def _viterbi_decode(self, feats):\n",
    "        backpointers = []\n",
    "\n",
    "        # 初始化viterbi的previous变量\n",
    "        init_vvars = torch.full((1, self.tagset_size), -10000.)\n",
    "        init_vvars[0][self.tag2idx[START_TAG]] = 0\n",
    "\n",
    "        previous = init_vvars\n",
    "        for obs in feats:\n",
    "            # 保存当前时间步的回溯指针\n",
    "            bptrs_t = []\n",
    "            # 保存当前时间步的viterbi变量\n",
    "            viterbivars_t = []\n",
    "\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # 维特比算法记录最优路径时只考虑上一步的分数以及上一步tag转移到当前tag的转移分数\n",
    "                # 并不取决与当前tag的发射分数\n",
    "                next_tag_var = previous + self.transitions[next_tag]\n",
    "                best_tag_id = argmax(next_tag_var)\n",
    "                bptrs_t.append(best_tag_id)\n",
    "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
    "            # 更新previous，加上当前tag的发射分数obs\n",
    "            previous = (torch.cat(viterbivars_t) + obs).view(1, -1)\n",
    "            # 回溯指针记录当前时间步各个tag来源前一步的tag\n",
    "            backpointers.append(bptrs_t)\n",
    "\n",
    "        # Transition to STOP_TAG\n",
    "        # 考虑转移到STOP_TAG的转移分数\n",
    "        terminal_var = previous + self.transitions[self.tag2idx[STOP_TAG]]\n",
    "        best_tag_id = argmax(terminal_var)\n",
    "        path_score = terminal_var[0][best_tag_id]\n",
    "\n",
    "        # 通过回溯指针解码出最优路径\n",
    "        best_path = [best_tag_id]\n",
    "        # best_tag_id作为线头，反向遍历backpointers找到最优路径\n",
    "        for bptrs_t in reversed(backpointers):\n",
    "            best_tag_id = bptrs_t[best_tag_id]\n",
    "            best_path.append(best_tag_id)\n",
    "        # 去除START_TAG\n",
    "        start = best_path.pop()\n",
    "        assert start == self.tag2idx[START_TAG]  # Sanity check\n",
    "        best_path.reverse()\n",
    "        return path_score, best_path\n",
    "\n",
    "    def neg_log_likelihood(self, sentence, tags):\n",
    "        # CRF损失函数由两部分组成，真实路径的分数和所有路径的总分数。\n",
    "        # 真实路径的分数应该是所有路径中分数最高的。\n",
    "        # log真实路径的分数/log所有可能路径的分数，越大越好，构造crf loss函数取反，loss越小越好\n",
    "        feats = self._get_lstm_features(sentence)\n",
    "        forward_score = self._forward_alg(feats)\n",
    "        gold_score = self._score_sentence(feats, tags)\n",
    "        return forward_score - gold_score\n",
    "\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        # 通过BiLSTM提取发射分数\n",
    "        lstm_feats = self._get_lstm_features(sentence)\n",
    "        # 根据发射分数以及转移分数，通过viterbi解码找到一条最优路径\n",
    "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
    "        return score, tag_seq"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Model, self).__init__()\n",
    "        self.config = config\n",
    "        # if config.embedding_pretrained is not None:\n",
    "        #     self.embedding = nn.Embedding.from_pretrained(config.embedding_pretrained,\n",
    "        #                                                   freeze=False)  # 表示训练过程词嵌入向量会更新\n",
    "        # else:\n",
    "        self.embedding = nn.Embedding(self.config.vocab_len, self.config.embedding_dim,\n",
    "                                      padding_idx=self.configs.word2idx['<PAD>'])  # PAD索引填充\n",
    "\n",
    "        if self.config.bidirectional:\n",
    "            self.num_directions = 2\n",
    "        else:\n",
    "            self.num_directions = 1\n",
    "\n",
    "\n",
    "        self.rnn = nn.LSTM(input_size=self.config.embedding_dim,\n",
    "                           hidden_size=self.config.hidden_size,\n",
    "                           num_layers=self.config.num_layers,\n",
    "                           batch_first=True,\n",
    "                           bidirectional=self.config.bidirectional)\n",
    "\n",
    "        self.tag2idx = configs.tag2idx\n",
    "\n",
    "        # 转换参数矩阵 输入i,j是得分从j转换到i\n",
    "        self.tagset_size = len(self.tag2idx)\n",
    "        # 将lstm的输出映射到标记空间\n",
    "        self.hidden2tag = nn.Linear(self.config.hidden_size*self.num_directions, self.tagset_size)  # -> (B, num_class+2)  加上了START END\n",
    "        self.crf = CRF(num_tags=self.tagset_size,batch_first=True)\n",
    "\n",
    "    def _forward_alg(self, feats):\n",
    "        # 使用前向算法计算分区函数\n",
    "        init_alphas = self._make_tensor(torch.full((1, self.tagset_size), -10000.))\n",
    "        # START_TAG 包含所有得分\n",
    "        init_alphas[0][self.tag2idx[START_TAG]] = 0.\n",
    "\n",
    "        # 包装一个变量 以便获得自动反向提升\n",
    "        forward_var = init_alphas\n",
    "\n",
    "        # 通过句子迭代\n",
    "        for feat in feats:\n",
    "            alphas_t = []  # the forward tensor at this timestep\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # 广播发射得分：无论之前的标记是怎样的都是相同的\n",
    "                emit_score = feat[next_tag].view(1, -1).expand(1, self.tagset_size)\n",
    "                # trans_score 的第i个条目是从i转移到next_tag的分数\n",
    "                trans_score = self.transitions[next_tag].view(1, -1)\n",
    "                # next_tag_var 的第i个条目是执行log-sum-exp之前的变（i -> next_tag）的值\n",
    "                next_tag_var = forward_var + trans_score + emit_score\n",
    "                # 此标记的转发变量是所有分数的log-sum-exp\n",
    "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
    "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
    "        terminal_var = forward_var + self.transitions[self.tag2idx[STOP_TAG]]\n",
    "        alpha = log_sum_exp(terminal_var)\n",
    "        return alpha\n",
    "\n",
    "    def _score_sentence(self, feats, tags):\n",
    "        # Give the score of a provided tag sequence\n",
    "        score = self._make_tensor(torch.zeros(1))\n",
    "        tags = self._make_tensor(torch.cat([self._make_tensor(torch.tensor([self.tag2idx[START_TAG]], dtype=torch.long)),tags]))\n",
    "        for i, feat in enumerate(feats):\n",
    "            score = score + self.transitions[tags[i+1], tags[i]]+feat[tags[i+1]]\n",
    "        score = score + self.transitions[self.tag2idx[STOP_TAG], tags[-1]]\n",
    "        return score\n",
    "\n",
    "    def _viterbi_decode(self, feats):\n",
    "        backpointers = []\n",
    "\n",
    "        # Initialize the viterbi variables in log space\n",
    "        init_vvars = self._make_tensor(torch.full((1, self.tagset_size), -10000.))\n",
    "        init_vvars[0][self.tag2idx[START_TAG]] = 0\n",
    "\n",
    "        # forward_var at step o holds the viterbi variables for step i-1\n",
    "        forward_var = init_vvars\n",
    "        for feat in feats:\n",
    "            bptrs_t = []  # hold the backpointers for this step\n",
    "            viterbivars_t = []  # holds the viterbi variables for this step\n",
    "\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # next_tag_var[i]保存上一步的标签i的viterbi变量\n",
    "                # 加上标签i转换到next_tag的分数 我们这里不包括emission分数 因为最大值不依赖于它们（在下面添加它们）\n",
    "                next_tag_var = forward_var + self.transitions[next_tag]\n",
    "                best_tag_id = argmax(next_tag_var)\n",
    "                bptrs_t.append(best_tag_id)\n",
    "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
    "            # 现在添加emission分数 并将forward_var分配给刚计算的viterbi变量集\n",
    "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
    "            backpointers.append(bptrs_t)\n",
    "\n",
    "        # 过渡到STOP_TAG\n",
    "        terminal_var = forward_var + self.transitions[self.tag2idx[STOP_TAG]]\n",
    "        best_tag_id = argmax(terminal_var)\n",
    "        path_score = terminal_var[0][best_tag_id]\n",
    "\n",
    "        # 按照后退指针解码最佳路径\n",
    "        best_path = [best_tag_id]\n",
    "        for bptrs_t in reversed(backpointers):\n",
    "            best_tag_id = bptrs_t[best_tag_id]\n",
    "            best_path.append(best_tag_id)\n",
    "        # 弹出开始标记（我们不想将器返回给调用者）\n",
    "        start = best_path.pop()\n",
    "        assert start == self.tag2idx[START_TAG]  # Sanity check\n",
    "        best_path.reverse()\n",
    "        return path_score, best_path\n",
    "\n",
    "    def _get_lstm_features(self, x):\n",
    "        # 数据预处理时，x被处理成是一个tuple,其内容是: (word, label).\n",
    "        # x:b_size\n",
    "        x = self.embedding(x)  # B -> (B, e_d)\n",
    "        x = x.unsqueeze(1)  # (B, e_b) -> (B, 1, e_b)\n",
    "        h_0, c_0 = self._init_hidden(batchs=x.size(0))\n",
    "        out, (hidden, c) = self.rnn(x,(h_0, c_0))  # out:(B, 1, num_directions*hidden_size) hidden:(num_layer*nun_directions, B,  hidden_size)\n",
    "        # out = out.squeeze(1)\n",
    "        # output is batch_first but hidden not\n",
    "        out = self.hidden2tag(out)  # (B,num_directions*hidden_size) -> (B, num_class)\n",
    "        out = out.transpose(0, 1)\n",
    "        return out\n",
    "\n",
    "    def neg_log_likelihood(self, x, tags):  # 损失函数\n",
    "        tags = tags.unsqueeze(0)\n",
    "        feats = self._get_lstm_features(x)\n",
    "        return -self.crf(feats, tags)\n",
    "\n",
    "\n",
    "\n",
    "    def _init_hidden(self, batchs):  # 初始化h_0和c_0 与GRU不同的是多了c_0（细胞状态）\n",
    "        h_0 = torch.zeros(self.config.num_layers*self.num_directions, batchs,  self.config.hidden_size)\n",
    "        c_0 = torch.zeros(self.config.num_layers*self.num_directions, batchs, self.config.hidden_size)\n",
    "        return self._make_tensor(h_0), self._make_tensor(c_0)\n",
    "\n",
    "    def _make_tensor(self, tensor):\n",
    "        # 函数说明： 将传入的tensor转移到cpu或gpu内\n",
    "\n",
    "        tensor_ret = tensor.to(self.config.device)\n",
    "        return tensor_ret\n",
    "\n",
    "    # def getTagLs(self, config):\n",
    "    #     tag_ls = config.class_ls\n",
    "    #     tag_ls.append(\"<START>\")\n",
    "    #     tag_ls.append(\"<STOP>\")\n",
    "    #     return tag_ls\n",
    "    #\n",
    "    # def getTagDic(self):\n",
    "    #     tag_dic = {}\n",
    "    #     for idx, label in enumerate(self.tag_ls):\n",
    "    #         tag_dic[label] = idx\n",
    "    #     return tag_dic\n",
    "    #\n",
    "    # def idx2Tag(self, idx):\n",
    "    #     return self.tag_ls[idx]\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 数据预处理时，x被处理成是一个tuple,其内容是: (word, label).\n",
    "        # x:b_size\n",
    "        lstm_feats = self._get_lstm_features(x)  # 获取BiLSTM的emission分数\n",
    "\n",
    "        out = self.crf.decode(lstm_feats)\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "from utils.param_configs import Configs\n",
    "configs = Configs()\n",
    "\n",
    "# 将训练集汉字使用数字表示\n",
    "# 为了方便调试，先用100条数据进行训练，调试好后可用全量数据进行训练\n",
    "training_data = training_data_txt[:]\n",
    "# --------------------------建立字典，字: idx-------------------------------------\n",
    "word2idx = {}\n",
    "# 训练集的\n",
    "for sentence, tags in training_data:\n",
    "    for word in sentence:\n",
    "        if word not in word2idx:\n",
    "            word2idx[word] = len(word2idx)\n",
    "# 测试集的\n",
    "testing_data = testing_data_txt\n",
    "for sentence in testing_data:\n",
    "    for word in sentence:\n",
    "        if word not in word2idx:\n",
    "            word2idx[word] = len(word2idx)\n",
    "\n",
    "# 加2个特殊字符\n",
    "word2idx['<UNK>'] = len(word2idx)\n",
    "word2idx['<PAD>'] = len(word2idx)\n",
    "\n",
    "configs.word2idx = word2idx\n",
    "# ------------------------------------------------------------------------------"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "(['交',\n  '行',\n  '1',\n  '4',\n  '年',\n  '用',\n  '过',\n  '，',\n  '半',\n  '年',\n  '准',\n  '备',\n  '提',\n  '额',\n  '，',\n  '却',\n  '直',\n  '接',\n  '被',\n  '降',\n  '到',\n  '1',\n  'Ｋ',\n  '，',\n  '半',\n  '年',\n  '期',\n  '间',\n  '只',\n  'T',\n  '过',\n  '一',\n  '次',\n  '三',\n  '千',\n  '，',\n  '其',\n  '它',\n  '全',\n  '部',\n  '真',\n  '实',\n  '消',\n  '费',\n  '，',\n  '第',\n  '六',\n  '个',\n  '月',\n  '的',\n  '时',\n  '候',\n  '为',\n  '了',\n  '增',\n  '加',\n  '评',\n  '分',\n  '提',\n  '额',\n  '，',\n  '还',\n  '特',\n  '意',\n  '分',\n  '期',\n  '两',\n  '万',\n  '，',\n  '但',\n  '降',\n  '额',\n  '后',\n  '电',\n  '话',\n  '投',\n  '诉',\n  '，',\n  '申',\n  '请',\n  '提',\n  '.',\n  '.',\n  '.'],\n ['B-BANK',\n  'I-BANK',\n  'O',\n  'O',\n  'O',\n  'O',\n  'O',\n  'O',\n  'O',\n  'O',\n  'O',\n  'O',\n  'B-COMMENTS_N',\n  'I-COMMENTS_N',\n  'O',\n  'O',\n  'O',\n  'O',\n  'O',\n  'B-COMMENTS_ADJ',\n  'I-COMMENTS_ADJ',\n  'O',\n  'O',\n  'O',\n  'O',\n  'O',\n  'O',\n  'O',\n  'O',\n  'O',\n  'O',\n  'O',\n  'O',\n  'O',\n  'O',\n  'O',\n  'O',\n  'O',\n  'O',\n  'O',\n  'O',\n  'O',\n  'B-COMMENTS_N',\n  'I-COMMENTS_N',\n  'O',\n  'O',\n  'O',\n  'O',\n  'O',\n  'O',\n  'O',\n  'O',\n  'O',\n  'O',\n  'B-COMMENTS_N',\n  'I-COMMENTS_N',\n  'O',\n  'O',\n  'B-COMMENTS_N',\n  'I-COMMENTS_N',\n  'O',\n  'O',\n  'O',\n  'O',\n  'B-PRODUCT',\n  'I-PRODUCT',\n  'O',\n  'O',\n  'O',\n  'O',\n  'B-COMMENTS_ADJ',\n  'O',\n  'O',\n  'O',\n  'O',\n  'O',\n  'O',\n  'O',\n  'O',\n  'O',\n  'O',\n  'O',\n  'O',\n  'O'])"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_txt[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from utils.data_process import create_data_loader\n",
    "train_data_loader = create_data_loader(training_data_txt, configs)\n",
    "test_data_loader = create_data_loader(testing_data_txt, configs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "5093"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testing_data_txt)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "START_TAG = \"<START>\"\n",
    "STOP_TAG = \"<STOP>\"\n",
    "EMBEDDING_DIM = 11\n",
    "HIDDEN_DIM = 6\n",
    "\n",
    "\n",
    "# 将训练集汉字使用数字表示\n",
    "# 为了方便调试，先用100条数据进行训练，调试好后可用全量数据进行训练\n",
    "training_data = training_data_txt[:]\n",
    "\n",
    "# --------------------------建立字典，字: idx-------------------------------------\n",
    "word2idx = {}\n",
    "# 训练集的\n",
    "for sentence, tags in training_data:\n",
    "    for word in sentence:\n",
    "        if word not in word2idx:\n",
    "            word2idx[word] = len(word2idx)\n",
    "# 测试集的\n",
    "testing_data = testing_data_txt\n",
    "for sentence in testing_data:\n",
    "    for word in sentence:\n",
    "        if word not in word2idx:\n",
    "            word2idx[word] = len(word2idx)\n",
    "# ------------------------------------------------------------------------------\n",
    "START_TAG = \"<START>\"\n",
    "STOP_TAG = \"<STOP>\"\n",
    "# 标签：idx\n",
    "tag2idx = { \"O\": 0, \"B-BANK\": 1, \"I-BANK\": 2, \"B-PRODUCT\":3,'I-PRODUCT':4,\n",
    "             'B-COMMENTS_N':5, 'I-COMMENTS_N':6, 'B-COMMENTS_ADJ':7,\n",
    "             'I-COMMENTS_ADJ':8, START_TAG: 9, STOP_TAG: 10}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "model = BiLSTM_CRF(len(word2idx), tag2idx, EMBEDDING_DIM, HIDDEN_DIM)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([84, 1, 6])\n",
      "torch.Size([84, 6])\n",
      "torch.Size([84, 11])\n",
      "(tensor(188.2542, grad_fn=<SelectBackward>), [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 8])\n"
     ]
    }
   ],
   "source": [
    "# 检查下模型输入输出\n",
    "precheck_sent = prepare_sequence(training_data[0][0], word2idx)\n",
    "precheck_tags = torch.tensor([tag2idx[t] for t in training_data[0][1]], dtype=torch.long)\n",
    "print(model(precheck_sent))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 0  epoch\n",
      "Time Taken: 0 seconds\n",
      "the 1  epoch\n",
      "Time Taken: 343 seconds\n",
      "the 2  epoch\n",
      "Time Taken: 686 seconds\n",
      "the 3  epoch\n",
      "Time Taken: 1029 seconds\n",
      "the 4  epoch\n",
      "Time Taken: 1372 seconds\n",
      "the 5  epoch\n",
      "Time Taken: 1716 seconds\n",
      "the 6  epoch\n",
      "Time Taken: 2060 seconds\n",
      "the 7  epoch\n",
      "Time Taken: 2404 seconds\n",
      "the 8  epoch\n",
      "Time Taken: 2748 seconds\n",
      "the 9  epoch\n",
      "Time Taken: 3092 seconds\n",
      "the 10  epoch\n",
      "Time Taken: 3436 seconds\n",
      "the 11  epoch\n",
      "Time Taken: 3779 seconds\n",
      "the 12  epoch\n",
      "Time Taken: 4123 seconds\n",
      "the 13  epoch\n",
      "Time Taken: 4467 seconds\n",
      "the 14  epoch\n",
      "Time Taken: 4811 seconds\n",
      "the 15  epoch\n",
      "Time Taken: 5155 seconds\n",
      "the 16  epoch\n",
      "Time Taken: 5499 seconds\n",
      "the 17  epoch\n",
      "Time Taken: 5843 seconds\n",
      "the 18  epoch\n",
      "Time Taken: 6186 seconds\n",
      "the 19  epoch\n",
      "Time Taken: 6530 seconds\n",
      "the 20  epoch\n",
      "Time Taken: 6874 seconds\n",
      "the 21  epoch\n",
      "Time Taken: 7218 seconds\n",
      "the 22  epoch\n",
      "Time Taken: 7562 seconds\n",
      "the 23  epoch\n",
      "Time Taken: 7906 seconds\n",
      "the 24  epoch\n",
      "Time Taken: 8249 seconds\n",
      "the 25  epoch\n",
      "Time Taken: 8593 seconds\n",
      "the 26  epoch\n",
      "Time Taken: 8937 seconds\n",
      "the 27  epoch\n",
      "Time Taken: 9280 seconds\n",
      "the 28  epoch\n",
      "Time Taken: 9624 seconds\n",
      "the 29  epoch\n",
      "Time Taken: 9968 seconds\n",
      "the 30  epoch\n",
      "Time Taken: 10311 seconds\n",
      "the 31  epoch\n",
      "Time Taken: 10655 seconds\n",
      "the 32  epoch\n",
      "Time Taken: 10999 seconds\n",
      "the 33  epoch\n",
      "Time Taken: 11343 seconds\n",
      "the 34  epoch\n",
      "Time Taken: 11686 seconds\n",
      "the 35  epoch\n",
      "Time Taken: 12030 seconds\n",
      "the 36  epoch\n",
      "Time Taken: 12375 seconds\n",
      "the 37  epoch\n",
      "Time Taken: 12718 seconds\n",
      "the 38  epoch\n",
      "Time Taken: 13062 seconds\n",
      "the 39  epoch\n",
      "Time Taken: 13406 seconds\n",
      "the 40  epoch\n",
      "Time Taken: 13750 seconds\n",
      "the 41  epoch\n",
      "Time Taken: 14093 seconds\n",
      "the 42  epoch\n",
      "Time Taken: 14437 seconds\n",
      "the 43  epoch\n",
      "Time Taken: 14780 seconds\n",
      "the 44  epoch\n",
      "Time Taken: 15124 seconds\n",
      "the 45  epoch\n",
      "Time Taken: 15468 seconds\n",
      "the 46  epoch\n",
      "Time Taken: 15811 seconds\n",
      "the 47  epoch\n",
      "Time Taken: 16155 seconds\n",
      "the 48  epoch\n",
      "Time Taken: 16499 seconds\n",
      "the 49  epoch\n",
      "Time Taken: 16842 seconds\n",
      "the 50  epoch\n",
      "Time Taken: 17185 seconds\n",
      "the 51  epoch\n",
      "Time Taken: 17529 seconds\n",
      "the 52  epoch\n",
      "Time Taken: 17872 seconds\n",
      "the 53  epoch\n",
      "Time Taken: 18216 seconds\n",
      "the 54  epoch\n",
      "Time Taken: 18560 seconds\n",
      "the 55  epoch\n",
      "Time Taken: 18905 seconds\n",
      "the 56  epoch\n",
      "Time Taken: 19248 seconds\n",
      "the 57  epoch\n",
      "Time Taken: 19592 seconds\n",
      "the 58  epoch\n",
      "Time Taken: 19935 seconds\n",
      "the 59  epoch\n",
      "Time Taken: 20279 seconds\n",
      "the 60  epoch\n",
      "Time Taken: 20622 seconds\n",
      "the 61  epoch\n",
      "Time Taken: 20966 seconds\n",
      "the 62  epoch\n",
      "Time Taken: 21309 seconds\n",
      "the 63  epoch\n",
      "Time Taken: 21653 seconds\n",
      "the 64  epoch\n",
      "Time Taken: 21996 seconds\n",
      "the 65  epoch\n",
      "Time Taken: 22340 seconds\n",
      "the 66  epoch\n",
      "Time Taken: 22683 seconds\n",
      "the 67  epoch\n",
      "Time Taken: 23027 seconds\n",
      "the 68  epoch\n",
      "Time Taken: 23370 seconds\n",
      "the 69  epoch\n",
      "Time Taken: 23714 seconds\n",
      "the 70  epoch\n",
      "Time Taken: 24057 seconds\n",
      "the 71  epoch\n",
      "Time Taken: 24401 seconds\n",
      "the 72  epoch\n",
      "Time Taken: 24744 seconds\n",
      "the 73  epoch\n",
      "Time Taken: 25087 seconds\n",
      "the 74  epoch\n",
      "Time Taken: 25430 seconds\n",
      "the 75  epoch\n",
      "Time Taken: 25774 seconds\n",
      "the 76  epoch\n",
      "Time Taken: 26117 seconds\n",
      "the 77  epoch\n",
      "Time Taken: 26460 seconds\n",
      "the 78  epoch\n",
      "Time Taken: 26804 seconds\n",
      "the 79  epoch\n",
      "Time Taken: 27147 seconds\n",
      "the 80  epoch\n",
      "Time Taken: 27491 seconds\n",
      "the 81  epoch\n",
      "Time Taken: 27834 seconds\n",
      "the 82  epoch\n",
      "Time Taken: 28177 seconds\n",
      "the 83  epoch\n",
      "Time Taken: 28521 seconds\n",
      "the 84  epoch\n",
      "Time Taken: 28864 seconds\n",
      "the 85  epoch\n",
      "Time Taken: 29208 seconds\n",
      "the 86  epoch\n",
      "Time Taken: 29551 seconds\n",
      "the 87  epoch\n",
      "Time Taken: 29895 seconds\n",
      "the 88  epoch\n",
      "Time Taken: 30238 seconds\n",
      "the 89  epoch\n",
      "Time Taken: 30582 seconds\n",
      "the 90  epoch\n",
      "Time Taken: 30925 seconds\n",
      "the 91  epoch\n",
      "Time Taken: 31269 seconds\n",
      "the 92  epoch\n",
      "Time Taken: 31612 seconds\n",
      "the 93  epoch\n",
      "Time Taken: 31956 seconds\n",
      "the 94  epoch\n",
      "Time Taken: 32299 seconds\n",
      "the 95  epoch\n",
      "Time Taken: 32643 seconds\n",
      "the 96  epoch\n",
      "Time Taken: 32986 seconds\n",
      "the 97  epoch\n",
      "Time Taken: 33330 seconds\n",
      "the 98  epoch\n",
      "Time Taken: 33673 seconds\n",
      "the 99  epoch\n",
      "Time Taken: 34017 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t = time.time()\n",
    "# Make sure prepare_sequence from earlier in the LSTM section is loaded\n",
    "for epoch in range(100):\n",
    "    print('the',epoch,' epoch')\n",
    "    print(f'Time Taken: {round(time.time()-t)} seconds')\n",
    "    for sentence, tags in training_data:\n",
    "        # 第一步，pytorch梯度累积，需要清零梯度\n",
    "        model.zero_grad()\n",
    "\n",
    "        # 第二步，将输入转化为tensors\n",
    "        sentence_in = prepare_sequence(sentence, word2idx)\n",
    "        targets = torch.tensor([tag2idx[t] for t in tags], dtype=torch.long)\n",
    "\n",
    "        # 进行前向计算，取出crf loss\n",
    "        loss = model.neg_log_likelihood(sentence_in, targets)\n",
    "\n",
    "        # 第四步，计算loss，梯度，通过optimier更新参数\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(341.8188), [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 0, 0, 0, 0, 0, 0, 0, 7, 8, 7, 8, 0, 0, 0])\n",
      "句子为： 我的怎么显示0.25费率，而且不管分多少期都一样费率，可惜只有69k\n",
      "实体标注结果为： O O O O O O O O O O O O O O O O O O B-COMMENTS_ADJ I-COMMENTS_ADJ O O O O O O O B-COMMENTS_ADJ I-COMMENTS_ADJ B-COMMENTS_ADJ I-COMMENTS_ADJ O O O\n"
     ]
    }
   ],
   "source": [
    "# 训练结束查看模型预测结果，对比观察模型是否学到\n",
    "# 标签：idx\n",
    "idx2tag = { 0:\"O\", 1:\"B-BANK\", 2:\"I-BANK\", 3:\"B-PRODUCT\",4:'I-PRODUCT',\n",
    "             5:'B-COMMENTS_N', 6:'I-COMMENTS_N', 7:'B-COMMENTS_ADJ',\n",
    "             8:'I-COMMENTS_ADJ', 9:START_TAG, 10:STOP_TAG}\n",
    "with torch.no_grad():\n",
    "    precheck_sent = prepare_sequence(training_data[4][0], word2idx)\n",
    "    print(model(precheck_sent))\n",
    "    a = model(precheck_sent) # model return score, tag_seq\n",
    "    # a = pd.Series(a)\n",
    "    print('句子为：', ''.join(training_data[4][0]))\n",
    "    print('实体标注结果为：', ' '.join([idx2tag[i] for i in a[1]]))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor(613.4543),\n [0,\n  0,\n  0,\n  0,\n  0,\n  1,\n  2,\n  2,\n  2,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  1,\n  2,\n  2,\n  2,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  1,\n  2,\n  2,\n  2,\n  2,\n  2,\n  0])"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}